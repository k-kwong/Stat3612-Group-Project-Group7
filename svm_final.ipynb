{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f57700c3",
      "metadata": {
        "id": "f57700c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns                       #visualisation\n",
        "import matplotlib.pyplot as plt             #visualisation\n",
        "%matplotlib inline\n",
        "sns.set(color_codes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MnFPu0331uCJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnFPu0331uCJ",
        "outputId": "d951c43e-f930-46b4-d1e6-fdc0eb4707e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# TODO: change this to yours ...\n",
        "DATA_DIR = \"/content/drive/MyDrive/untitled folder\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9akIQncEPuVy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9akIQncEPuVy",
        "outputId": "54dc48c7-af6d-4e8c-ad74-e3de25b32df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. Data Loading ---\n",
            "CSV data loaded successfully.\n",
            "EHR pickle data loaded successfully.\n",
            "EHR data keys verified: ['feat_dict', 'feature_cols', 'cat_idxs', 'cat_dims', 'demo_cols', 'icd_cols', 'lab_cols', 'med_cols']\n",
            "\n",
            "--- 2. Feature Aggregation ---\n",
            "Aggregating features for train_df using 'mean' method...\n",
            "Aggregation complete for train_df. Shape: (8234, 171)\n",
            "Aggregating features for val_df using 'mean' method...\n",
            "Aggregation complete for val_df. Shape: (2788, 171)\n",
            "Aggregating features for test_df using 'mean' method...\n",
            "Aggregation complete for test_df. Shape: (2741, 171)\n",
            "\n",
            "--- 3. Initial Feature Filtering ---\n",
            "Calculating feature statistics...\n",
            "Feature statistics calculated.\n",
            "Applying initial filtering based on thresholds...\n",
            "Identified 40 features to remove based on initial filtering.\n",
            "Number of features remaining after initial filtering: 131\n",
            "Shapes after initial filtering - Train X: (8234, 131), Val X: (2788, 131), Test X: (2741, 131)\n",
            "\n",
            "--- 4. Collinearity Handling ---\n",
            "Found 2 pairs of features with correlation > 0.9\n",
            "Removed 1 features due to collinearity.\n",
            "Number of features remaining after collinearity handling: 130\n",
            "Shapes after final filtering - Train X: (8234, 130), Val X: (2788, 130), Test X: (2741, 130)\n",
            "\n",
            "--- 5. Feature Scaling ---\n",
            "Scaling complete.\n",
            "\n",
            "Preprocessing finished. Ready for modeling using:\n",
            " X_train_scaled ((8234, 130))\n",
            " y_train ((8234,))\n",
            " X_val_scaled ((2788, 130))\n",
            " y_val ((2788,))\n",
            " X_test_scaled ((2741, 130))\n",
            " final_features (130 names)\n",
            " train_ids (8234), val_ids (2788), test_ids (2741)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pointbiserialr, chi2_contingency\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "\n",
        "# Suppress specific FutureWarning from pandas about inplace modification\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "# REVIEW POINT: Ensure DATA_DIR is correct for your Kaggle environment\n",
        "# Example for Kaggle: DATA_DIR = \"/kaggle/input/your-dataset-folder-name\"\n",
        "\n",
        "# REVIEW POINT: Choose Aggregation Method ('last', 'mean', 'max')\n",
        "AGGREGATION_METHOD = 'mean'\n",
        "\n",
        "# REVIEW POINT: Define Filtering Thresholds\n",
        "# Adjust these values to experiment with initial feature removal\n",
        "INITIAL_FILTER_THRESHOLDS = {\n",
        "    'zero_ratio': { # Remove if ZERO RATIO is ABOVE this\n",
        "        'icd': 0.95,\n",
        "        'med': 0.95,\n",
        "        'lab': 0.95,\n",
        "        'demo': 0.90\n",
        "    },\n",
        "    'std_dev': 0.001, # Remove if STD DEV is BELOW this (only for non-ICD)\n",
        "    'correlation': {\n",
        "        'min_abs_corr': 0.001, # Remove if ABS CORRELATION/CHI2 is BELOW this\n",
        "        'max_pvalue': 0.05 # Remove if P-VALUE is ABOVE this\n",
        "    }\n",
        "}\n",
        "\n",
        "# REVIEW POINT: Define Collinearity Threshold\n",
        "# Adjust this value to experiment with removing correlated features\n",
        "COLLINEARITY_THRESHOLD = 0.9\n",
        "\n",
        "# REVIEW POINT: Define Clinically Relevant Features\n",
        "# These features will NOT be removed by the initial filtering step\n",
        "# Review this list for appropriateness to your specific problem/data\n",
        "CLINICALLY_RELEVANT = [\n",
        "    'Creatinine Blood', 'Hemoglobin Blood', 'Hematocrit Blood',\n",
        "    'Potassium Blood', 'Sodium Blood', 'Glucose Blood',\n",
        "    'Troponin T Blood', 'Platelet Count Blood', 'Eosinophils Blood',\n",
        "    'pH Urine', 'pO2 Blood', 'pCO2 Blood', 'Anion Gap Blood',\n",
        "    'I10-I16', 'N17-N19', 'J09-J18', 'E70-E88', 'I30-I52',\n",
        "    'J40-J47', 'B20-B20',\n",
        "    'ANTICOAGULANTS', 'ANTIBIOTICS', 'IMMUNOSUPPRESSANTS',\n",
        "    'ANTIINFLAM.TUMOR NECROSIS FACTOR INHIBITING AGENTS'\n",
        "]\n",
        "# --- End Configuration ---\n",
        "\n",
        "\n",
        "# === 1. Data Loading ===\n",
        "print(\"--- 1. Data Loading ---\")\n",
        "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
        "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
        "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
        "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
        "\n",
        "try:\n",
        "    train_df = pd.read_csv(train_csv_file)\n",
        "    val_df = pd.read_csv(val_csv_file)\n",
        "    test_df = pd.read_csv(test_csv_file)\n",
        "    print(\"CSV data loaded successfully.\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading CSV files: {e}\")\n",
        "    print(f\"Please ensure DATA_DIR '{DATA_DIR}' is correct and files exist.\")\n",
        "    raise\n",
        "\n",
        "try:\n",
        "    with open(ehr_pkl_file, 'rb') as f:\n",
        "        ehr_data = pd.read_pickle(f)\n",
        "    print(\"EHR pickle data loaded successfully.\")\n",
        "    # Verify necessary keys exist\n",
        "    required_keys = ['feat_dict', 'feature_cols', 'cat_idxs', 'cat_dims', 'demo_cols', 'icd_cols', 'lab_cols', 'med_cols']\n",
        "    for key in required_keys:\n",
        "        if key not in ehr_data:\n",
        "            raise KeyError(f\"Missing required key in ehr_data: '{key}'\")\n",
        "    print(f\"EHR data keys verified: {list(ehr_data.keys())}\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading pickle file: {e}\")\n",
        "    print(f\"Please ensure ehr_pkl_file path is correct.\")\n",
        "    raise\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred loading pickle file: {e}\")\n",
        "    raise\n",
        "\n",
        "# === 2. Feature Aggregation ===\n",
        "print(\"\\n--- 2. Feature Aggregation ---\")\n",
        "def aggregate_admission_data(df, ehr_data_dict, aggregation, df_name=\"DataFrame\", has_labels=True):\n",
        "    \"\"\"Aggregate features per admission with optional labels\"\"\"\n",
        "    print(f\"Aggregating features for {df_name} using '{aggregation}' method...\")\n",
        "    admission_ids = df['id'].unique()\n",
        "    X = []\n",
        "    y = [] if has_labels else None\n",
        "    processed_ids = [] # Keep track of IDs for which we successfully generated features\n",
        "\n",
        "    feature_dict = ehr_data_dict.get('feat_dict', {})\n",
        "\n",
        "    for adm_id in admission_ids:\n",
        "        # Ensure adm_id exists in the feature dictionary\n",
        "        if adm_id not in feature_dict:\n",
        "            print(f\"Warning: Admission ID {adm_id} not found in ehr_data['feat_dict']. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        adm_features = feature_dict[adm_id]\n",
        "\n",
        "        # Ensure adm_features is not empty or invalid\n",
        "        if not isinstance(adm_features, np.ndarray) or adm_features.size == 0:\n",
        "            print(f\"Warning: Invalid or empty features found for Admission ID {adm_id}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Apply aggregation\n",
        "        try:\n",
        "            if aggregation == 'last':\n",
        "                features = adm_features[-1]\n",
        "            elif aggregation == 'mean':\n",
        "                features = np.mean(adm_features, axis=0)\n",
        "            elif aggregation == 'max':\n",
        "                features = np.max(adm_features, axis=0)\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid aggregation method: {aggregation}\")\n",
        "        except IndexError:\n",
        "             print(f\"Warning: IndexError during '{aggregation}' aggregation for Admission ID {adm_id} (likely empty sequence). Skipping.\")\n",
        "             continue\n",
        "        except Exception as e:\n",
        "             print(f\"Warning: Error during aggregation for Admission ID {adm_id}: {e}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        # Check if aggregated features have the expected dimension (should match feature_cols length)\n",
        "        expected_dim = len(ehr_data_dict.get('feature_cols', []))\n",
        "        if expected_dim > 0 and features.shape[0] != expected_dim:\n",
        "             print(f\"Warning: Feature dimension mismatch for {adm_id}. Expected {expected_dim}, got {features.shape[0]}. Skipping.\")\n",
        "             continue\n",
        "\n",
        "        X.append(features)\n",
        "        processed_ids.append(adm_id) # Add ID only if features were processed\n",
        "\n",
        "        # Get label if needed\n",
        "        if has_labels:\n",
        "            adm_rows = df[df['id'] == adm_id]\n",
        "            if not adm_rows.empty:\n",
        "                y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
        "            else:\n",
        "                # This case indicates an ID was in ehr_data but not the corresponding CSV\n",
        "                # Remove the features added for this ID as we can't get its label\n",
        "                print(f\"Warning: Admission ID {adm_id} found in ehr_dict but not in {df_name}. Cannot get label. Removing feature vector.\")\n",
        "                X.pop()\n",
        "                processed_ids.pop()\n",
        "\n",
        "    X_np = np.array(X)\n",
        "    y_np = np.array(y) if has_labels and y is not None else None # Ensure y is converted only if it exists\n",
        "\n",
        "    # Final check for consistency between processed IDs and output shapes\n",
        "    if X_np.shape[0] != len(processed_ids):\n",
        "         print(f\"Error: Mismatch between number of processed IDs ({len(processed_ids)}) and feature rows ({X_np.shape[0]}) for {df_name}.\")\n",
        "         # Handle error appropriately - maybe raise exception or return empty arrays\n",
        "         raise RuntimeError(\"Inconsistent number of samples after aggregation.\")\n",
        "    if has_labels and y_np is not None and y_np.shape[0] != len(processed_ids):\n",
        "         print(f\"Error: Mismatch between number of processed IDs ({len(processed_ids)}) and labels ({y_np.shape[0]}) for {df_name}.\")\n",
        "         raise RuntimeError(\"Inconsistent number of labels after aggregation.\")\n",
        "\n",
        "\n",
        "    print(f\"Aggregation complete for {df_name}. Shape: {X_np.shape}\")\n",
        "    return X_np, y_np, processed_ids # Return processed IDs as well\n",
        "\n",
        "# Perform aggregation\n",
        "X_train, y_train, train_ids = aggregate_admission_data(train_df, ehr_data, AGGREGATION_METHOD, df_name=\"train_df\")\n",
        "X_val, y_val, val_ids = aggregate_admission_data(val_df, ehr_data, AGGREGATION_METHOD, df_name=\"val_df\")\n",
        "X_test, _, test_ids = aggregate_admission_data(test_df, ehr_data, AGGREGATION_METHOD, df_name=\"test_df\", has_labels=False)\n",
        "\n",
        "# Verify non-empty results\n",
        "if X_train.size == 0 or X_val.size == 0 or X_test.size == 0:\n",
        "    raise ValueError(\"Aggregation resulted in empty datasets. Check input data and aggregation logic.\")\n",
        "\n",
        "\n",
        "# === 3. Initial Feature Filtering ===\n",
        "print(\"\\n--- 3. Initial Feature Filtering ---\")\n",
        "feature_names = list(ehr_data[\"feature_cols\"]) # Get the original list of feature names\n",
        "\n",
        "# Check for consistency\n",
        "if len(feature_names) != X_train.shape[1]:\n",
        "    raise ValueError(f\"Mismatch: {len(feature_names)} feature names in 'feature_cols' but {X_train.shape[1]} columns in aggregated data.\")\n",
        "\n",
        "# --- Calculate Stats (Std Dev, Correlation, Zero Ratio) ---\n",
        "print(\"Calculating feature statistics...\")\n",
        "std_devs = np.std(X_train, axis=0)\n",
        "std_df = pd.DataFrame({\"Feature\": feature_names, \"Std_Dev\": std_devs})\n",
        "\n",
        "correlations = []\n",
        "p_values = []\n",
        "for i in range(X_train.shape[1]):\n",
        "    feature_name = feature_names[i]\n",
        "    if std_devs[i] == 0:\n",
        "        correlations.append(0)\n",
        "        p_values.append(1)\n",
        "        continue\n",
        "    if feature_name in ehr_data[\"icd_cols\"]:\n",
        "        contingency_table = pd.crosstab(X_train[:, i], y_train)\n",
        "        try:\n",
        "            chi2, pval, _, _ = chi2_contingency(contingency_table)\n",
        "            correlations.append(chi2)\n",
        "            p_values.append(pval)\n",
        "        except ValueError: correlations.append(0); p_values.append(1)\n",
        "    else:\n",
        "        try:\n",
        "            if np.std(y_train) == 0: corr, pval = 0, 1\n",
        "            else: corr, pval = pointbiserialr(X_train[:, i], y_train)\n",
        "            if np.isnan(corr): corr, pval = 0, 1\n",
        "            correlations.append(corr); p_values.append(pval)\n",
        "        except ValueError: correlations.append(0); p_values.append(1)\n",
        "\n",
        "corr_df = pd.DataFrame({\"Feature\": feature_names, \"Correlation\": correlations, \"P-value\": p_values})\n",
        "corr_df[\"Abs_Correlation\"] = np.abs(corr_df[\"Correlation\"])\n",
        "\n",
        "# Function to analyze zero-value ratio\n",
        "def analyze_zeros(ehr_data, feature_cols_category):\n",
        "    all_zeros_info = []\n",
        "    all_feature_names_list = ehr_data[\"feature_cols\"]\n",
        "    for feature_name in feature_cols_category:\n",
        "        try: feature_idx = all_feature_names_list.index(feature_name)\n",
        "        except ValueError: continue\n",
        "        zeros_count = np.sum([np.sum(matrix[:, feature_idx] == 0) for matrix in ehr_data[\"feat_dict\"].values()])\n",
        "        total_values = sum([matrix.shape[0] for matrix in ehr_data[\"feat_dict\"].values()])\n",
        "        zero_ratio = (zeros_count / total_values) if total_values > 0 else 0\n",
        "        all_zeros_info.append((feature_name, zero_ratio))\n",
        "    return pd.DataFrame(all_zeros_info, columns=[\"Feature\", \"Zero_Ratio\"])\n",
        "\n",
        "demo_zero_df = analyze_zeros(ehr_data, ehr_data[\"demo_cols\"])\n",
        "icd_zero_df = analyze_zeros(ehr_data, ehr_data[\"icd_cols\"])\n",
        "lab_zero_df = analyze_zeros(ehr_data, ehr_data[\"lab_cols\"])\n",
        "med_zero_df = analyze_zeros(ehr_data, ehr_data[\"med_cols\"])\n",
        "zero_df = pd.concat([demo_zero_df, icd_zero_df, lab_zero_df, med_zero_df], ignore_index=True)\n",
        "\n",
        "combined_df = pd.merge(pd.merge(std_df, corr_df, on='Feature'), zero_df, on='Feature', how='left')\n",
        "combined_df['Zero_Ratio'].fillna(0, inplace=True)\n",
        "print(\"Feature statistics calculated.\")\n",
        "\n",
        "# --- Apply Filtering ---\n",
        "print(\"Applying initial filtering based on thresholds...\")\n",
        "feature_types = {\n",
        "    'icd': ehr_data[\"icd_cols\"], 'med': ehr_data[\"med_cols\"],\n",
        "    'lab': ehr_data[\"lab_cols\"], 'demo': ehr_data[\"demo_cols\"]\n",
        "}\n",
        "features_to_remove = []\n",
        "for ftype, cols in feature_types.items():\n",
        "    for feature in cols:\n",
        "        if feature in CLINICALLY_RELEVANT: continue\n",
        "        row = combined_df[combined_df['Feature'] == feature]\n",
        "        if row.empty: continue\n",
        "        row = row.iloc[0]\n",
        "        check_variance = ftype not in ['icd']\n",
        "        sig_threshold = INITIAL_FILTER_THRESHOLDS['correlation']['max_pvalue']\n",
        "        effect_size = row['Abs_Correlation']\n",
        "        remove_condition = (\n",
        "            (row['Zero_Ratio'] > INITIAL_FILTER_THRESHOLDS['zero_ratio'][ftype]) and\n",
        "            (not check_variance or row['Std_Dev'] < INITIAL_FILTER_THRESHOLDS['std_dev']) and\n",
        "            (effect_size < INITIAL_FILTER_THRESHOLDS['correlation']['min_abs_corr']) and\n",
        "            (row['P-value'] > sig_threshold)\n",
        "        )\n",
        "        if remove_condition: features_to_remove.append(feature)\n",
        "\n",
        "remove_indices = sorted([i for i, feature in enumerate(feature_names) if feature in features_to_remove], reverse=True)\n",
        "print(f\"Identified {len(features_to_remove)} features to remove based on initial filtering.\")\n",
        "\n",
        "# Remove features from data and names list\n",
        "X_train_filtered = np.delete(X_train, remove_indices, axis=1)\n",
        "X_val_filtered = np.delete(X_val, remove_indices, axis=1)\n",
        "X_test_filtered = np.delete(X_test, remove_indices, axis=1)\n",
        "remaining_features_after_initial = [f for i, f in enumerate(feature_names) if i not in remove_indices]\n",
        "\n",
        "print(f\"Number of features remaining after initial filtering: {len(remaining_features_after_initial)}\")\n",
        "print(f\"Shapes after initial filtering - Train X: {X_train_filtered.shape}, Val X: {X_val_filtered.shape}, Test X: {X_test_filtered.shape}\")\n",
        "\n",
        "\n",
        "# === 4. Collinearity Handling ===\n",
        "print(\"\\n--- 4. Collinearity Handling ---\")\n",
        "# --- Function Definitions ---\n",
        "def find_collinear_features(X, feature_names, threshold):\n",
        "    \"\"\"Identify feature pairs with correlation > threshold\"\"\"\n",
        "    if X.shape[1] < 2: return pd.DataFrame(columns=['Feature1', 'Feature2', 'Correlation']) # Handle case with < 2 features\n",
        "    # Scale data before calculating correlations for stability\n",
        "    scaler_coll = StandardScaler()\n",
        "    X_scaled_coll = scaler_coll.fit_transform(X)\n",
        "    corr_matrix = pd.DataFrame(X_scaled_coll, columns=feature_names).corr().abs()\n",
        "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    collinear_pairs = [(feature_names[i], feature_names[j], corr_matrix.iloc[i,j])\n",
        "                      for i,j in zip(*np.where(upper > threshold))]\n",
        "    return pd.DataFrame(collinear_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
        "\n",
        "def select_non_collinear_features(X, y, feature_names, clinically_relevant, corr_threshold):\n",
        "    \"\"\" Selects features, removing one from each highly correlated pair based on importance. \"\"\"\n",
        "    if X.shape[1] < 2: return list(range(X.shape[1])), feature_names # Return all if < 2 features\n",
        "\n",
        "    # Find collinear pairs\n",
        "    collinear_df = find_collinear_features(X, feature_names, corr_threshold)\n",
        "    print(f\"Found {len(collinear_df)} pairs of features with correlation > {corr_threshold}\")\n",
        "    if collinear_df.empty:\n",
        "        print(\"No highly collinear features found to remove.\")\n",
        "        return list(range(X.shape[1])), feature_names # Return all indices and names\n",
        "\n",
        "    # Calculate Mutual Information\n",
        "    try:\n",
        "        # Ensure y has more than one class for MI calculation\n",
        "        if len(np.unique(y)) > 1:\n",
        "            importance = mutual_info_classif(X, y, random_state=42)\n",
        "            importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "        else:\n",
        "            print(\"Warning: Target variable has only one class. Cannot calculate mutual information. Using fallback for selection.\")\n",
        "            importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': 0}) # Dummy importance\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating mutual information: {e}. Using fallback for selection.\")\n",
        "        importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': 0}) # Dummy importance\n",
        "\n",
        "    clinically_relevant_remaining = [f for f in clinically_relevant if f in feature_names]\n",
        "    features_to_keep = set(clinically_relevant_remaining)\n",
        "    features_to_remove_coll = set() # Track features explicitly marked for removal\n",
        "\n",
        "    # Iterate through collinear pairs to decide which one to remove\n",
        "    processed_in_pair = set() # Track features already handled in a pair\n",
        "    for _, row in collinear_df.iterrows():\n",
        "        f1, f2 = row['Feature1'], row['Feature2']\n",
        "\n",
        "        # If either feature was already removed in a previous pair comparison, skip\n",
        "        if f1 in features_to_remove_coll or f2 in features_to_remove_coll: continue\n",
        "        # If both already decided to keep (e.g. both clinical), skip\n",
        "        if f1 in features_to_keep and f2 in features_to_keep: continue\n",
        "\n",
        "        # Prioritize clinically relevant\n",
        "        if f1 in clinically_relevant_remaining:\n",
        "            features_to_keep.add(f1)\n",
        "            if f2 not in clinically_relevant_remaining: features_to_remove_coll.add(f2)\n",
        "            continue\n",
        "        elif f2 in clinically_relevant_remaining:\n",
        "            features_to_keep.add(f2)\n",
        "            if f1 not in clinically_relevant_remaining: features_to_remove_coll.add(f1)\n",
        "            continue\n",
        "        # If neither is clinically relevant, compare importance\n",
        "        else:\n",
        "            try:\n",
        "                imp1 = importance_df.loc[importance_df['Feature'] == f1, 'Importance'].iloc[0]\n",
        "                imp2 = importance_df.loc[importance_df['Feature'] == f2, 'Importance'].iloc[0]\n",
        "                if imp1 >= imp2: # Keep f1\n",
        "                    features_to_keep.add(f1)\n",
        "                    features_to_remove_coll.add(f2)\n",
        "                else: # Keep f2\n",
        "                    features_to_keep.add(f2)\n",
        "                    features_to_remove_coll.add(f1)\n",
        "            except IndexError:\n",
        "                 print(f\"Warning: Feature {f1} or {f2} not found in importance df. Keeping first ({f1}).\")\n",
        "                 features_to_keep.add(f1)\n",
        "                 features_to_remove_coll.add(f2)\n",
        "\n",
        "    # Final list of features to keep\n",
        "    final_feature_set = (set(feature_names) - features_to_remove_coll)\n",
        "\n",
        "    # Get indices based on the feature_names list passed to this function\n",
        "    keep_indices = sorted([i for i, f in enumerate(feature_names) if f in final_feature_set])\n",
        "    final_feature_names = [feature_names[i] for i in keep_indices]\n",
        "\n",
        "    print(f\"Removed {len(features_to_remove_coll)} features due to collinearity.\")\n",
        "    return keep_indices, final_feature_names\n",
        "# --- End of Function Definitions ---\n",
        "\n",
        "# Apply collinearity filtering\n",
        "keep_indices_final, final_features = select_non_collinear_features(\n",
        "    X_train_filtered,\n",
        "    y_train,\n",
        "    remaining_features_after_initial, # Pass the list of names corresponding to X_train_filtered\n",
        "    CLINICALLY_RELEVANT,\n",
        "    corr_threshold=COLLINEARITY_THRESHOLD\n",
        ")\n",
        "\n",
        "# Select final features based on the returned indices relative to X_train_filtered\n",
        "X_train_final = X_train_filtered[:, keep_indices_final]\n",
        "X_val_final = X_val_filtered[:, keep_indices_final]\n",
        "X_test_final = X_test_filtered[:, keep_indices_final]\n",
        "\n",
        "print(f\"Number of features remaining after collinearity handling: {len(final_features)}\")\n",
        "print(f\"Shapes after final filtering - Train X: {X_train_final.shape}, Val X: {X_val_final.shape}, Test X: {X_test_final.shape}\")\n",
        "\n",
        "\n",
        "# === 5. Feature Scaling ===\n",
        "print(\"\\n--- 5. Feature Scaling ---\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_final)\n",
        "X_val_scaled = scaler.transform(X_val_final)\n",
        "X_test_scaled = scaler.transform(X_test_final)\n",
        "print(\"Scaling complete.\")\n",
        "print(\"\\nPreprocessing finished. Ready for modeling using:\")\n",
        "print(f\" X_train_scaled ({X_train_scaled.shape})\")\n",
        "print(f\" y_train ({y_train.shape})\")\n",
        "print(f\" X_val_scaled ({X_val_scaled.shape})\")\n",
        "print(f\" y_val ({y_val.shape})\")\n",
        "print(f\" X_test_scaled ({X_test_scaled.shape})\")\n",
        "print(f\" final_features ({len(final_features)} names)\")\n",
        "print(f\" train_ids ({len(train_ids)}), val_ids ({len(val_ids)}), test_ids ({len(test_ids)})\") # IDs corresponding to rows\n",
        "\n",
        "# Optional: Save the processed data if needed for separate modeling notebooks\n",
        "# np.savez('processed_data.npz',\n",
        "#          X_train_scaled=X_train_scaled, y_train=y_train, train_ids=train_ids,\n",
        "#          X_val_scaled=X_val_scaled, y_val=y_val, val_ids=val_ids,\n",
        "#          X_test_scaled=X_test_scaled, test_ids=test_ids,\n",
        "#          final_features=final_features)\n",
        "# print(\"\\nProcessed data saved to processed_data.npz\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CijJXHYccF2V",
      "metadata": {
        "id": "CijJXHYccF2V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYmRaPs1Z4gu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYmRaPs1Z4gu",
        "outputId": "b6472f4b-43ea-44ca-97be-118f01b19157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training SVM Classifier model...\n",
            "(Note: SVM training with probability=True can be slow)...\n",
            "Training complete. Time taken: 37.07 seconds\n",
            "\n",
            "Evaluating SVM model on validation set...\n",
            "Validation Accuracy (SVM): 0.7848\n",
            "Validation AUC-ROC (SVM): 0.7342\n",
            "\n",
            "Validation Classification Report (SVM):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.88      0.85      0.87      2307\n",
            "        True       0.39      0.45      0.42       481\n",
            "\n",
            "    accuracy                           0.78      2788\n",
            "   macro avg       0.64      0.65      0.64      2788\n",
            "weighted avg       0.80      0.78      0.79      2788\n",
            "\n",
            "\n",
            "Predicting probabilities on the test set using SVM...\n",
            "Prediction complete.\n",
            "Number of test predictions: 2741\n",
            "Number of test IDs: 2741\n",
            "Test IDs and predictions match.\n",
            "\n",
            "Submission file for SVM created successfully at: submission_svm.csv\n",
            "Submission file head (SVM):\n",
            "                  id  readmitted_within_30days\n",
            "0  16026764_21404901                  0.160818\n",
            "1  18463717_24608289                  0.562249\n",
            "2  16914073_28403863                  0.213977\n",
            "3  17119335_28722930                  0.057360\n",
            "4  10599715_20082646                  0.080888\n"
          ]
        }
      ],
      "source": [
        "# 1. Import necessary libraries (if not already imported)\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler # Ensure scaler is available if needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time # To time the training\n",
        "\n",
        "# Assume X_train_scaled, y_train, X_val_scaled, y_val, X_test_scaled, test_ids\n",
        "# and test_csv_file are available from the previous preprocessing cell.\n",
        "\n",
        "# 2. Train an SVM Classifier Model\n",
        "# Using kernel='rbf' is common.\n",
        "# C is the regularization parameter.\n",
        "# probability=True allows predict_proba but slows down training considerably.\n",
        "print(\"Training SVM Classifier model...\")\n",
        "print(\"(Note: SVM training with probability=True can be slow)...\")\n",
        "svm_model = SVC(\n",
        "    random_state=42,\n",
        "    C=1.0,                 # Default regularization parameter\n",
        "    kernel='rbf',          # Common kernel choice\n",
        "    gamma='scale',         # Default gamma value\n",
        "    probability=True,      # Enable probability estimates\n",
        "    class_weight='balanced' # Handle class imbalance\n",
        ")\n",
        "\n",
        "# Time the training\n",
        "start_time = time.time()\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "end_time = time.time()\n",
        "print(f\"Training complete. Time taken: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# 3. Evaluate the Model on the Validation Set\n",
        "print(\"\\nEvaluating SVM model on validation set...\")\n",
        "y_val_pred_svm = svm_model.predict(X_val_scaled)\n",
        "y_val_pred_proba_svm = svm_model.predict_proba(X_val_scaled)[:, 1] # Probabilities\n",
        "\n",
        "accuracy_svm = accuracy_score(y_val, y_val_pred_svm)\n",
        "auc_roc_svm = roc_auc_score(y_val, y_val_pred_proba_svm)\n",
        "\n",
        "print(f\"Validation Accuracy (SVM): {accuracy_svm:.4f}\")\n",
        "print(f\"Validation AUC-ROC (SVM): {auc_roc_svm:.4f}\")\n",
        "print(\"\\nValidation Classification Report (SVM):\")\n",
        "try:\n",
        "    print(classification_report(y_val, y_val_pred_svm))\n",
        "except ValueError as e:\n",
        "    print(f\"Could not generate classification report: {e}\")\n",
        "\n",
        "\n",
        "# 4. Predict Probabilities on the Test Set using SVM\n",
        "print(\"\\nPredicting probabilities on the test set using SVM...\")\n",
        "y_test_pred_proba_svm = svm_model.predict_proba(X_test_scaled)[:, 1]\n",
        "print(\"Prediction complete.\")\n",
        "\n",
        "# 5. Create Submission File for SVM\n",
        "# Use the test_ids returned by the aggregation function\n",
        "print(f\"Number of test predictions: {len(y_test_pred_proba_svm)}\")\n",
        "print(f\"Number of test IDs: {len(test_ids)}\")\n",
        "\n",
        "if len(test_ids) != len(y_test_pred_proba_svm):\n",
        "    print(f\"Error: Length mismatch! Test IDs ({len(test_ids)}) vs Predictions ({len(y_test_pred_proba_svm)})\")\n",
        "    # Attempt fallback using unique IDs from original test_df if lengths match\n",
        "    try:\n",
        "        original_test_df = pd.read_csv(test_csv_file)\n",
        "        fallback_test_ids = original_test_df['id'].unique()\n",
        "        if len(fallback_test_ids) == len(y_test_pred_proba_svm):\n",
        "            print(\"Warning: Using unique IDs from original test_df as fallback.\")\n",
        "            test_ids_to_use = fallback_test_ids\n",
        "        else:\n",
        "             raise ValueError(\"Cannot reliably create submission file due to ID mismatch even with fallback.\")\n",
        "    except Exception as fallback_e:\n",
        "         print(f\"Fallback ID generation failed: {fallback_e}\")\n",
        "         raise ValueError(\"Cannot reliably create submission file due to ID mismatch.\")\n",
        "else:\n",
        "    print(\"Test IDs and predictions match.\")\n",
        "    test_ids_to_use = test_ids # Use the IDs from aggregation\n",
        "\n",
        "\n",
        "try:\n",
        "    submission_df_svm = pd.DataFrame({\n",
        "        'id': test_ids_to_use,\n",
        "        'readmitted_within_30days': y_test_pred_proba_svm\n",
        "    })\n",
        "\n",
        "    # Define the output path\n",
        "    output_path_svm = \"submission_svm.csv\" # Give it a distinct name\n",
        "\n",
        "    # Save the submission file\n",
        "    submission_df_svm.to_csv(output_path_svm, index=False)\n",
        "\n",
        "    print(f\"\\nSubmission file for SVM created successfully at: {output_path_svm}\")\n",
        "    print(\"Submission file head (SVM):\")\n",
        "    print(submission_df_svm.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while preparing the submission file: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f_jqvxUScG8o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_jqvxUScG8o",
        "outputId": "e54d2870-c626-4e2b-c869-7df94c35a6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Starting SVM Experiment Loop ===\n",
            "Testing Aggregations: ['last', 'mean', 'max']\n",
            "Testing Collinearity Thresholds: [0.6, 0.7, 0.8, 0.9]\n",
            "WARNING: This may take a long time due to SVM training!\n",
            "\n",
            "--- Running Preprocessing: Aggregation='last', Collinearity Threshold=0.6 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 11 pairs with correlation > 0.6\n",
            "Collinearity handling removed 6 features.\n",
            "Final feature count: 125\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='last', Coll='0.6')...\n",
            "SVM Training complete. Time: 35.13s\n",
            "Validation AUC-ROC: 0.7264\n",
            "--- Finished run for Agg='last', Coll='0.6'. Total time: 99.06s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='last', Collinearity Threshold=0.7 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 9 pairs with correlation > 0.7\n",
            "Collinearity handling removed 5 features.\n",
            "Final feature count: 126\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='last', Coll='0.7')...\n",
            "SVM Training complete. Time: 35.70s\n",
            "Validation AUC-ROC: 0.7264\n",
            "--- Finished run for Agg='last', Coll='0.7'. Total time: 99.38s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='last', Collinearity Threshold=0.8 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 4 pairs with correlation > 0.8\n",
            "Collinearity handling removed 2 features.\n",
            "Final feature count: 129\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='last', Coll='0.8')...\n",
            "SVM Training complete. Time: 31.74s\n",
            "Validation AUC-ROC: 0.7294\n",
            "--- Finished run for Agg='last', Coll='0.8'. Total time: 93.12s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='last', Collinearity Threshold=0.9 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 1 pairs with correlation > 0.9\n",
            "Collinearity handling removed 0 features.\n",
            "Final feature count: 131\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='last', Coll='0.9')...\n",
            "SVM Training complete. Time: 33.96s\n",
            "Validation AUC-ROC: 0.7298\n",
            "--- Finished run for Agg='last', Coll='0.9'. Total time: 97.83s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='mean', Collinearity Threshold=0.6 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 9 pairs with correlation > 0.6\n",
            "Collinearity handling removed 6 features.\n",
            "Final feature count: 125\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='mean', Coll='0.6')...\n",
            "SVM Training complete. Time: 33.88s\n",
            "Validation AUC-ROC: 0.7338\n",
            "--- Finished run for Agg='mean', Coll='0.6'. Total time: 96.30s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='mean', Collinearity Threshold=0.7 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 6 pairs with correlation > 0.7\n",
            "Collinearity handling removed 3 features.\n",
            "Final feature count: 128\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='mean', Coll='0.7')...\n",
            "SVM Training complete. Time: 31.06s\n",
            "Validation AUC-ROC: 0.7336\n",
            "--- Finished run for Agg='mean', Coll='0.7'. Total time: 93.32s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='mean', Collinearity Threshold=0.8 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 6 pairs with correlation > 0.8\n",
            "Collinearity handling removed 3 features.\n",
            "Final feature count: 128\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='mean', Coll='0.8')...\n",
            "SVM Training complete. Time: 30.78s\n",
            "Validation AUC-ROC: 0.7336\n",
            "--- Finished run for Agg='mean', Coll='0.8'. Total time: 92.50s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='mean', Collinearity Threshold=0.9 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 2 pairs with correlation > 0.9\n",
            "Collinearity handling removed 1 features.\n",
            "Final feature count: 130\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='mean', Coll='0.9')...\n",
            "SVM Training complete. Time: 33.35s\n",
            "Validation AUC-ROC: 0.7342\n",
            "--- Finished run for Agg='mean', Coll='0.9'. Total time: 98.10s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='max', Collinearity Threshold=0.6 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 2 pairs with correlation > 0.6\n",
            "Collinearity handling removed 1 features.\n",
            "Final feature count: 130\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='max', Coll='0.6')...\n",
            "SVM Training complete. Time: 36.92s\n",
            "Validation AUC-ROC: 0.6715\n",
            "--- Finished run for Agg='max', Coll='0.6'. Total time: 99.83s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='max', Collinearity Threshold=0.7 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 1 pairs with correlation > 0.7\n",
            "Collinearity handling removed 0 features.\n",
            "Final feature count: 131\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='max', Coll='0.7')...\n",
            "SVM Training complete. Time: 37.29s\n",
            "Validation AUC-ROC: 0.6779\n",
            "--- Finished run for Agg='max', Coll='0.7'. Total time: 99.56s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='max', Collinearity Threshold=0.8 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 1 pairs with correlation > 0.8\n",
            "Collinearity handling removed 0 features.\n",
            "Final feature count: 131\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='max', Coll='0.8')...\n",
            "SVM Training complete. Time: 37.31s\n",
            "Validation AUC-ROC: 0.6779\n",
            "--- Finished run for Agg='max', Coll='0.8'. Total time: 102.36s ---\n",
            "\n",
            "--- Running Preprocessing: Aggregation='max', Collinearity Threshold=0.9 ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregation complete.\n",
            "Performing initial filtering...\n",
            "Initial filtering removed 40 features. 131 remaining.\n",
            "Performing collinearity handling...\n",
            "Found 0 pairs with correlation > 0.9\n",
            "Final feature count: 131\n",
            "Scaling features...\n",
            "Preprocessing complete.\n",
            "\n",
            "Training SVM (Agg='max', Coll='0.9')...\n",
            "SVM Training complete. Time: 36.75s\n",
            "Validation AUC-ROC: 0.6779\n",
            "--- Finished run for Agg='max', Coll='0.9'. Total time: 96.58s ---\n",
            "\n",
            "=== Experiment Results ===\n",
            "   Aggregation  Collinearity Threshold Validation AUC SVM Train Time (s)  \\\n",
            "0         last                     0.6         0.7264              35.13   \n",
            "1         last                     0.7         0.7264              35.70   \n",
            "2         last                     0.8         0.7294              31.74   \n",
            "3         last                     0.9         0.7298              33.96   \n",
            "4         mean                     0.6         0.7338              33.88   \n",
            "5         mean                     0.7         0.7336              31.06   \n",
            "6         mean                     0.8         0.7336              30.78   \n",
            "7         mean                     0.9         0.7342              33.35   \n",
            "8          max                     0.6         0.6715              36.92   \n",
            "9          max                     0.7         0.6779              37.29   \n",
            "10         max                     0.8         0.6779              37.31   \n",
            "11         max                     0.9         0.6779              36.75   \n",
            "\n",
            "    Preprocessing Success  \n",
            "0                    True  \n",
            "1                    True  \n",
            "2                    True  \n",
            "3                    True  \n",
            "4                    True  \n",
            "5                    True  \n",
            "6                    True  \n",
            "7                    True  \n",
            "8                    True  \n",
            "9                    True  \n",
            "10                   True  \n",
            "11                   True  \n",
            "==========================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.stats import pointbiserialr, chi2_contingency\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import warnings\n",
        "\n",
        "# Suppress specific FutureWarning from pandas\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "# REVIEW POINT: Ensure DATA_DIR is correct for your Kaggle environment\n",
        "\n",
        "# --- Parameters to Test ---\n",
        "AGGREGATION_METHODS_TO_TEST = ['last', 'mean', 'max']\n",
        "COLLINEARITY_THRESHOLDS_TO_TEST = [0.6, 0.7, 0.8, 0.9] # Add or remove values as needed\n",
        "\n",
        "# --- Fixed Parameters for this Experiment ---\n",
        "INITIAL_FILTER_THRESHOLDS = { # Using original strict thresholds for consistency in this test\n",
        "    'zero_ratio': {'icd': 0.95, 'med': 0.95, 'lab': 0.95, 'demo': 0.90},\n",
        "    'std_dev': 0.001,\n",
        "    'correlation': {'min_abs_corr': 0.001, 'max_pvalue': 0.05}\n",
        "}\n",
        "CLINICALLY_RELEVANT = [\n",
        "    'Creatinine Blood', 'Hemoglobin Blood', 'Hematocrit Blood',\n",
        "    'Potassium Blood', 'Sodium Blood', 'Glucose Blood',\n",
        "    'Troponin T Blood', 'Platelet Count Blood', 'Eosinophils Blood',\n",
        "    'pH Urine', 'pO2 Blood', 'pCO2 Blood', 'Anion Gap Blood',\n",
        "    'I10-I16', 'N17-N19', 'J09-J18', 'E70-E88', 'I30-I52',\n",
        "    'J40-J47', 'B20-B20',\n",
        "    'ANTICOAGULANTS', 'ANTIBIOTICS', 'IMMUNOSUPPRESSANTS',\n",
        "    'ANTIINFLAM.TUMOR NECROSIS FACTOR INHIBITING AGENTS'\n",
        "]\n",
        "# Define file paths globally within the cell\n",
        "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
        "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
        "test_csv_file = os.path.join(DATA_DIR, \"test.csv\") # Needed for ID mapping if generating submissions later\n",
        "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
        "\n",
        "# --- Preprocessing Function ---\n",
        "def run_preprocessing(aggregation_method, collinearity_threshold):\n",
        "    \"\"\"Runs the entire preprocessing pipeline and returns scaled data.\"\"\"\n",
        "    print(f\"\\n--- Running Preprocessing: Aggregation='{aggregation_method}', Collinearity Threshold={collinearity_threshold} ---\")\n",
        "    try:\n",
        "        # 1. Load Data (inside function to ensure fresh load each time)\n",
        "        print(\"Loading data...\")\n",
        "        train_df = pd.read_csv(train_csv_file)\n",
        "        val_df = pd.read_csv(val_csv_file)\n",
        "        test_df = pd.read_csv(test_csv_file) # Load test_df for consistency check if needed later\n",
        "        with open(ehr_pkl_file, 'rb') as f:\n",
        "            ehr_data = pd.read_pickle(f)\n",
        "        print(\"Data loaded.\")\n",
        "\n",
        "        # 2. Feature Aggregation\n",
        "        print(\"Aggregating features...\")\n",
        "        # (Using the robust aggregate_admission_data function from previous consolidated code)\n",
        "        # --- Start: aggregate_admission_data definition ---\n",
        "        def aggregate_admission_data(df, ehr_data_dict, aggregation, df_name=\"DataFrame\", has_labels=True):\n",
        "            admission_ids = df['id'].unique()\n",
        "            X = []\n",
        "            y = [] if has_labels else None\n",
        "            processed_ids = []\n",
        "            feature_dict = ehr_data_dict.get('feat_dict', {})\n",
        "            feature_cols = ehr_data_dict.get('feature_cols', [])\n",
        "            expected_dim = len(feature_cols)\n",
        "\n",
        "            for adm_id in admission_ids:\n",
        "                if adm_id not in feature_dict: continue\n",
        "                adm_features = feature_dict[adm_id]\n",
        "                if not isinstance(adm_features, np.ndarray) or adm_features.size == 0: continue\n",
        "\n",
        "                try:\n",
        "                    if aggregation == 'last': features = adm_features[-1]\n",
        "                    elif aggregation == 'mean': features = np.mean(adm_features, axis=0)\n",
        "                    elif aggregation == 'max': features = np.max(adm_features, axis=0)\n",
        "                    else: raise ValueError(f\"Invalid aggregation method: {aggregation}\")\n",
        "                except Exception: continue # Skip if aggregation fails\n",
        "\n",
        "                if expected_dim > 0 and features.shape[0] != expected_dim: continue\n",
        "\n",
        "                X.append(features)\n",
        "                processed_ids.append(adm_id)\n",
        "\n",
        "                if has_labels:\n",
        "                    adm_rows = df[df['id'] == adm_id]\n",
        "                    if not adm_rows.empty: y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
        "                    else: X.pop(); processed_ids.pop() # Remove if label missing\n",
        "\n",
        "            X_np = np.array(X)\n",
        "            y_np = np.array(y) if has_labels and y is not None else None\n",
        "            if X_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent samples after aggregation.\")\n",
        "            if has_labels and y_np is not None and y_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent labels after aggregation.\")\n",
        "            return X_np, y_np, processed_ids\n",
        "        # --- End: aggregate_admission_data definition ---\n",
        "\n",
        "        X_train, y_train, _ = aggregate_admission_data(train_df, ehr_data, aggregation_method, df_name=\"train_df\")\n",
        "        X_val, y_val, _ = aggregate_admission_data(val_df, ehr_data, aggregation_method, df_name=\"val_df\")\n",
        "        # We only need X_train/y_train and X_val/y_val for this experiment\n",
        "        if X_train.size == 0 or X_val.size == 0 or y_train is None or y_val is None:\n",
        "             raise ValueError(\"Aggregation resulted in empty train/val data or labels.\")\n",
        "        print(\"Aggregation complete.\")\n",
        "\n",
        "        # 3. Initial Feature Filtering\n",
        "        print(\"Performing initial filtering...\")\n",
        "        feature_names = list(ehr_data[\"feature_cols\"])\n",
        "        if len(feature_names) != X_train.shape[1]: raise ValueError(\"Feature name/column mismatch.\")\n",
        "        std_devs = np.std(X_train, axis=0)\n",
        "        std_df = pd.DataFrame({\"Feature\": feature_names, \"Std_Dev\": std_devs})\n",
        "        # (Correlation/P-value calculation logic...)\n",
        "        correlations = []; p_values = []\n",
        "        for i in range(X_train.shape[1]):\n",
        "            fname = feature_names[i]\n",
        "            if std_devs[i] == 0: corr, pval = 0, 1\n",
        "            elif fname in ehr_data[\"icd_cols\"]:\n",
        "                try: corr, pval, _, _ = chi2_contingency(pd.crosstab(X_train[:, i], y_train));\n",
        "                except ValueError: corr, pval = 0, 1\n",
        "            else:\n",
        "                try:\n",
        "                    if np.std(y_train)==0: corr, pval = 0, 1\n",
        "                    else: corr, pval = pointbiserialr(X_train[:, i], y_train)\n",
        "                    if np.isnan(corr): corr, pval = 0, 1\n",
        "                except ValueError: corr, pval = 0, 1\n",
        "            correlations.append(corr); p_values.append(pval)\n",
        "        corr_df = pd.DataFrame({\"Feature\": feature_names, \"Correlation\": correlations, \"P-value\": p_values})\n",
        "        corr_df[\"Abs_Correlation\"] = np.abs(corr_df[\"Correlation\"])\n",
        "        # (Zero ratio calculation logic...)\n",
        "        def analyze_zeros(ehr_data, feature_cols_category):\n",
        "             all_zeros_info = []\n",
        "             all_feature_names_list = ehr_data[\"feature_cols\"]\n",
        "             for fname in feature_cols_category:\n",
        "                 try: fidx = all_feature_names_list.index(fname)\n",
        "                 except ValueError: continue\n",
        "                 zeros_count = np.sum([np.sum(m[:, fidx] == 0) for m in ehr_data[\"feat_dict\"].values()])\n",
        "                 total_values = sum([m.shape[0] for m in ehr_data[\"feat_dict\"].values()])\n",
        "                 zero_ratio = (zeros_count / total_values) if total_values > 0 else 0\n",
        "                 all_zeros_info.append((fname, zero_ratio))\n",
        "             return pd.DataFrame(all_zeros_info, columns=[\"Feature\", \"Zero_Ratio\"])\n",
        "        demo_zero_df = analyze_zeros(ehr_data, ehr_data[\"demo_cols\"])\n",
        "        icd_zero_df = analyze_zeros(ehr_data, ehr_data[\"icd_cols\"])\n",
        "        lab_zero_df = analyze_zeros(ehr_data, ehr_data[\"lab_cols\"])\n",
        "        med_zero_df = analyze_zeros(ehr_data, ehr_data[\"med_cols\"])\n",
        "        zero_df = pd.concat([demo_zero_df, icd_zero_df, lab_zero_df, med_zero_df], ignore_index=True)\n",
        "        combined_df = pd.merge(pd.merge(std_df, corr_df, on='Feature'), zero_df, on='Feature', how='left')\n",
        "        combined_df['Zero_Ratio'].fillna(0, inplace=True)\n",
        "        # (Filtering logic...)\n",
        "        feature_types = {'icd': ehr_data[\"icd_cols\"], 'med': ehr_data[\"med_cols\"], 'lab': ehr_data[\"lab_cols\"], 'demo': ehr_data[\"demo_cols\"]}\n",
        "        features_to_remove = []\n",
        "        for ftype, cols in feature_types.items():\n",
        "            for feature in cols:\n",
        "                if feature in CLINICALLY_RELEVANT: continue\n",
        "                row = combined_df[combined_df['Feature'] == feature]\n",
        "                if row.empty: continue\n",
        "                row = row.iloc[0]\n",
        "                check_variance = ftype not in ['icd']\n",
        "                sig_threshold = INITIAL_FILTER_THRESHOLDS['correlation']['max_pvalue']\n",
        "                effect_size = row['Abs_Correlation']\n",
        "                remove_condition = (\n",
        "                    (row['Zero_Ratio'] > INITIAL_FILTER_THRESHOLDS['zero_ratio'][ftype]) and\n",
        "                    (not check_variance or row['Std_Dev'] < INITIAL_FILTER_THRESHOLDS['std_dev']) and\n",
        "                    (effect_size < INITIAL_FILTER_THRESHOLDS['correlation']['min_abs_corr']) and\n",
        "                    (row['P-value'] > sig_threshold)\n",
        "                )\n",
        "                if remove_condition: features_to_remove.append(feature)\n",
        "        remove_indices = sorted([i for i, feature in enumerate(feature_names) if feature in features_to_remove], reverse=True)\n",
        "        X_train_filtered = np.delete(X_train, remove_indices, axis=1)\n",
        "        X_val_filtered = np.delete(X_val, remove_indices, axis=1)\n",
        "        remaining_features_after_initial = [f for i, f in enumerate(feature_names) if i not in remove_indices]\n",
        "        print(f\"Initial filtering removed {len(features_to_remove)} features. {len(remaining_features_after_initial)} remaining.\")\n",
        "\n",
        "        # 4. Collinearity Handling\n",
        "        print(\"Performing collinearity handling...\")\n",
        "        # (Using find_collinear_features and select_non_collinear_features definitions from previous consolidated code)\n",
        "        # --- Start: Collinearity function definitions ---\n",
        "        def find_collinear_features(X, feature_names, threshold):\n",
        "            if X.shape[1] < 2: return pd.DataFrame(columns=['Feature1', 'Feature2', 'Correlation'])\n",
        "            scaler_coll = StandardScaler()\n",
        "            X_scaled_coll = scaler_coll.fit_transform(X)\n",
        "            corr_matrix = pd.DataFrame(X_scaled_coll, columns=feature_names).corr().abs()\n",
        "            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "            collinear_pairs = [(feature_names[i], feature_names[j], corr_matrix.iloc[i,j])\n",
        "                              for i,j in zip(*np.where(upper > threshold))]\n",
        "            return pd.DataFrame(collinear_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
        "\n",
        "        def select_non_collinear_features(X, y, feature_names, clinically_relevant, corr_threshold):\n",
        "            if X.shape[1] < 2: return list(range(X.shape[1])), feature_names\n",
        "            collinear_df = find_collinear_features(X, feature_names, corr_threshold)\n",
        "            print(f\"Found {len(collinear_df)} pairs with correlation > {corr_threshold}\")\n",
        "            if collinear_df.empty: return list(range(X.shape[1])), feature_names\n",
        "            try:\n",
        "                if len(np.unique(y)) > 1:\n",
        "                    importance = mutual_info_classif(X, y, random_state=42)\n",
        "                    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
        "                else: importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': 0})\n",
        "            except Exception: importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': 0})\n",
        "            clinically_relevant_remaining = [f for f in clinically_relevant if f in feature_names]\n",
        "            features_to_keep = set(clinically_relevant_remaining)\n",
        "            features_to_remove_coll = set()\n",
        "            for _, row in collinear_df.iterrows():\n",
        "                f1, f2 = row['Feature1'], row['Feature2']\n",
        "                if f1 in features_to_remove_coll or f2 in features_to_remove_coll: continue\n",
        "                if f1 in features_to_keep and f2 in features_to_keep: continue\n",
        "                if f1 in clinically_relevant_remaining:\n",
        "                    if f2 not in clinically_relevant_remaining: features_to_remove_coll.add(f2)\n",
        "                    continue\n",
        "                elif f2 in clinically_relevant_remaining:\n",
        "                    if f1 not in clinically_relevant_remaining: features_to_remove_coll.add(f1)\n",
        "                    continue\n",
        "                else:\n",
        "                    try:\n",
        "                        imp1 = importance_df.loc[importance_df['Feature'] == f1, 'Importance'].iloc[0]\n",
        "                        imp2 = importance_df.loc[importance_df['Feature'] == f2, 'Importance'].iloc[0]\n",
        "                        if imp1 >= imp2: features_to_remove_coll.add(f2)\n",
        "                        else: features_to_remove_coll.add(f1)\n",
        "                    except IndexError: features_to_remove_coll.add(f2) # Fallback: remove second one\n",
        "            final_feature_set = (set(feature_names) - features_to_remove_coll)\n",
        "            keep_indices = sorted([i for i, f in enumerate(feature_names) if f in final_feature_set])\n",
        "            final_feature_names = [feature_names[i] for i in keep_indices]\n",
        "            print(f\"Collinearity handling removed {len(features_to_remove_coll)} features.\")\n",
        "            return keep_indices, final_feature_names\n",
        "        # --- End: Collinearity function definitions ---\n",
        "\n",
        "        keep_indices_final, final_features = select_non_collinear_features(\n",
        "            X_train_filtered, y_train, remaining_features_after_initial,\n",
        "            CLINICALLY_RELEVANT, corr_threshold=collinearity_threshold\n",
        "        )\n",
        "        if not final_features: raise ValueError(\"Collinearity handling removed all features.\")\n",
        "\n",
        "        X_train_final = X_train_filtered[:, keep_indices_final]\n",
        "        X_val_final = X_val_filtered[:, keep_indices_final]\n",
        "        print(f\"Final feature count: {len(final_features)}\")\n",
        "\n",
        "        # 5. Feature Scaling\n",
        "        print(\"Scaling features...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train_final)\n",
        "        X_val_scaled = scaler.transform(X_val_final)\n",
        "        print(\"Preprocessing complete.\")\n",
        "\n",
        "        return X_train_scaled, y_train, X_val_scaled, y_val, True # Success\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"!!! Preprocessing failed for Agg='{aggregation_method}', Coll='{collinearity_threshold}': {e} !!!\")\n",
        "        return None, None, None, None, False # Failure\n",
        "\n",
        "# --- Experiment Loop ---\n",
        "results_list = []\n",
        "print(\"\\n=== Starting SVM Experiment Loop ===\")\n",
        "print(f\"Testing Aggregations: {AGGREGATION_METHODS_TO_TEST}\")\n",
        "print(f\"Testing Collinearity Thresholds: {COLLINEARITY_THRESHOLDS_TO_TEST}\")\n",
        "print(\"WARNING: This may take a long time due to SVM training!\")\n",
        "\n",
        "for agg_method in AGGREGATION_METHODS_TO_TEST:\n",
        "    for coll_thresh in COLLINEARITY_THRESHOLDS_TO_TEST:\n",
        "        run_start_time = time.time()\n",
        "\n",
        "        # Run preprocessing\n",
        "        X_train_s, y_train_p, X_val_s, y_val_p, success = run_preprocessing(agg_method, coll_thresh)\n",
        "\n",
        "        val_auc = None # Default AUC if preprocessing or training fails\n",
        "        train_time = None\n",
        "\n",
        "        if success:\n",
        "            # Initialize and Train SVM\n",
        "            print(f\"\\nTraining SVM (Agg='{agg_method}', Coll='{coll_thresh}')...\")\n",
        "            svm_model = SVC(random_state=42, C=1.0, kernel='rbf', gamma='scale',\n",
        "                            probability=True, class_weight='balanced')\n",
        "            try:\n",
        "                svm_start_time = time.time()\n",
        "                svm_model.fit(X_train_s, y_train_p)\n",
        "                svm_end_time = time.time()\n",
        "                train_time = svm_end_time - svm_start_time\n",
        "                print(f\"SVM Training complete. Time: {train_time:.2f}s\")\n",
        "\n",
        "                # Evaluate\n",
        "                y_val_pred_proba_svm = svm_model.predict_proba(X_val_s)[:, 1]\n",
        "                val_auc = roc_auc_score(y_val_p, y_val_pred_proba_svm)\n",
        "                print(f\"Validation AUC-ROC: {val_auc:.4f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"!!! SVM training/evaluation failed: {e} !!!\")\n",
        "                val_auc = None # Mark as failed\n",
        "                train_time = None\n",
        "        else:\n",
        "             print(\"Skipping SVM training due to preprocessing failure.\")\n",
        "\n",
        "\n",
        "        # Store results\n",
        "        results_list.append({\n",
        "            'Aggregation': agg_method,\n",
        "            'Collinearity Threshold': coll_thresh,\n",
        "            'Validation AUC': val_auc,\n",
        "            'SVM Train Time (s)': train_time,\n",
        "            'Preprocessing Success': success\n",
        "        })\n",
        "        run_end_time = time.time()\n",
        "        print(f\"--- Finished run for Agg='{agg_method}', Coll='{coll_thresh}'. Total time: {run_end_time - run_start_time:.2f}s ---\")\n",
        "\n",
        "\n",
        "# --- Display Results ---\n",
        "print(\"\\n=== Experiment Results ===\")\n",
        "results_df = pd.DataFrame(results_list)\n",
        "# Format AUC nicely\n",
        "results_df['Validation AUC'] = results_df['Validation AUC'].map('{:.4f}'.format, na_action='ignore')\n",
        "results_df['SVM Train Time (s)'] = results_df['SVM Train Time (s)'].map('{:.2f}'.format, na_action='ignore')\n",
        "\n",
        "print(results_df)\n",
        "print(\"==========================\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "snmWVB7IjQJ6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snmWVB7IjQJ6",
        "outputId": "0a0049d6-bbe7-448d-805c-6e060164934a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Evaluating SVM on Aggregated Data (No Filtering) ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregating features for train_df using 'mean' method...\n",
            "Aggregation complete for train_df. Shape: (8234, 171)\n",
            "Aggregating features for val_df using 'mean' method...\n",
            "Aggregation complete for val_df. Shape: (2788, 171)\n",
            "Aggregating features for test_df using 'mean' method...\n",
            "Aggregation complete for test_df. Shape: (2741, 171)\n",
            "Aggregated shapes: Train=(8234, 171), Val=(2788, 171), Test=(2741, 171)\n",
            "Scaling aggregated features...\n",
            "Scaling complete.\n",
            "Training SVM on aggregated (unfiltered) data...\n",
            "(Note: SVM training with probability=True can be slow)...\n",
            "Training complete. Time: 39.63s\n",
            "\n",
            "Evaluating SVM model on validation set (aggregated data)...\n",
            "Validation Accuracy (SVM Aggregated): 0.7816\n",
            "Validation AUC-ROC (SVM Aggregated): 0.7343\n",
            "\n",
            "Validation Classification Report (SVM Aggregated):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.84      0.86      2307\n",
            "        True       0.39      0.49      0.44       481\n",
            "\n",
            "    accuracy                           0.78      2788\n",
            "   macro avg       0.64      0.67      0.65      2788\n",
            "weighted avg       0.80      0.78      0.79      2788\n",
            "\n",
            "\n",
            "Predicting probabilities on the test set (aggregated data)...\n",
            "Prediction complete.\n",
            "Number of test predictions: 2741\n",
            "Number of test IDs: 2741\n",
            "Test IDs and predictions match.\n",
            "\n",
            "Submission file for SVM (Aggregated) created successfully at: submission_svm_aggregated.csv\n",
            "Submission file head (SVM Aggregated):\n",
            "                  id  readmitted_within_30days\n",
            "0  16026764_21404901                  0.158666\n",
            "1  18463717_24608289                  0.543315\n",
            "2  16914073_28403863                  0.208099\n",
            "3  17119335_28722930                  0.057497\n",
            "4  10599715_20082646                  0.081485\n",
            "\n",
            "--- Comparison Point ---\n",
            "Validation AUC for SVM on Aggregated Data (171 features): 0.7343\n",
            "Compare this to the AUC score you obtained for SVM after full preprocessing.\n",
            "--------------------------\n"
          ]
        }
      ],
      "source": [
        "# 1. Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Suppress specific FutureWarning from pandas\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "AGGREGATION_METHOD = 'mean' # Or 'mean', 'max' if you want to test that aggregation\n",
        "# Define file paths globally\n",
        "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
        "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
        "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
        "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
        "\n",
        "# --- Function Definition (Aggregation Step Only) ---\n",
        "def aggregate_admission_data(df, ehr_data_dict, aggregation, df_name=\"DataFrame\", has_labels=True):\n",
        "    \"\"\"Aggregate features per admission with optional labels\"\"\"\n",
        "    print(f\"Aggregating features for {df_name} using '{aggregation}' method...\")\n",
        "    admission_ids = df['id'].unique()\n",
        "    X = []\n",
        "    y = [] if has_labels else None\n",
        "    processed_ids = []\n",
        "    feature_dict = ehr_data_dict.get('feat_dict', {})\n",
        "    feature_cols = ehr_data_dict.get('feature_cols', [])\n",
        "    expected_dim = len(feature_cols)\n",
        "\n",
        "    for adm_id in admission_ids:\n",
        "        if adm_id not in feature_dict: continue\n",
        "        adm_features = feature_dict[adm_id]\n",
        "        if not isinstance(adm_features, np.ndarray) or adm_features.size == 0: continue\n",
        "        try:\n",
        "            if aggregation == 'last': features = adm_features[-1]\n",
        "            elif aggregation == 'mean': features = np.mean(adm_features, axis=0)\n",
        "            elif aggregation == 'max': features = np.max(adm_features, axis=0)\n",
        "            else: raise ValueError(f\"Invalid aggregation method: {aggregation}\")\n",
        "        except Exception: continue\n",
        "        if expected_dim > 0 and features.shape[0] != expected_dim: continue\n",
        "        X.append(features)\n",
        "        processed_ids.append(adm_id)\n",
        "        if has_labels:\n",
        "            adm_rows = df[df['id'] == adm_id]\n",
        "            if not adm_rows.empty: y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
        "            else: X.pop(); processed_ids.pop()\n",
        "    X_np = np.array(X); y_np = np.array(y) if has_labels and y is not None else None\n",
        "    if X_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent samples after aggregation.\")\n",
        "    if has_labels and y_np is not None and y_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent labels after aggregation.\")\n",
        "    print(f\"Aggregation complete for {df_name}. Shape: {X_np.shape}\")\n",
        "    return X_np, y_np, processed_ids\n",
        "\n",
        "# --- Main Logic ---\n",
        "results_agg_only = {}\n",
        "\n",
        "try:\n",
        "    # === Run SVM on Aggregated Data (No Filtering) ===\n",
        "    print(\"--- Evaluating SVM on Aggregated Data (No Filtering) ---\")\n",
        "    # 1a. Load Data\n",
        "    print(\"Loading data...\")\n",
        "    train_df = pd.read_csv(train_csv_file)\n",
        "    val_df = pd.read_csv(val_csv_file)\n",
        "    test_df = pd.read_csv(test_csv_file) # Load test_df for potential submission later\n",
        "    with open(ehr_pkl_file, 'rb') as f:\n",
        "        ehr_data = pd.read_pickle(f)\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    # 1b. Aggregate Features\n",
        "    print(\"Aggregating features...\")\n",
        "    X_train_agg, y_train_agg, train_ids_agg = aggregate_admission_data(train_df, ehr_data, AGGREGATION_METHOD, df_name=\"train_df\")\n",
        "    X_val_agg, y_val_agg, val_ids_agg = aggregate_admission_data(val_df, ehr_data, AGGREGATION_METHOD, df_name=\"val_df\")\n",
        "    X_test_agg, _, test_ids_agg = aggregate_admission_data(test_df, ehr_data, AGGREGATION_METHOD, df_name=\"test_df\", has_labels=False)\n",
        "    print(f\"Aggregated shapes: Train={X_train_agg.shape}, Val={X_val_agg.shape}, Test={X_test_agg.shape}\")\n",
        "\n",
        "    # 1c. Scale Aggregated Features\n",
        "    print(\"Scaling aggregated features...\")\n",
        "    scaler_agg = StandardScaler()\n",
        "    X_train_scaled_agg = scaler_agg.fit_transform(X_train_agg)\n",
        "    X_val_scaled_agg = scaler_agg.transform(X_val_agg)\n",
        "    X_test_scaled_agg = scaler_agg.transform(X_test_agg) # Scale test set too\n",
        "    print(\"Scaling complete.\")\n",
        "\n",
        "    # 1d. Train and Evaluate SVM on Aggregated Data\n",
        "    print(\"Training SVM on aggregated (unfiltered) data...\")\n",
        "    print(\"(Note: SVM training with probability=True can be slow)...\")\n",
        "    svm_model_agg = SVC(random_state=42, C=1.0, kernel='rbf', gamma='scale',\n",
        "                        probability=True, class_weight='balanced')\n",
        "    start_time = time.time()\n",
        "    svm_model_agg.fit(X_train_scaled_agg, y_train_agg)\n",
        "    end_time = time.time()\n",
        "    train_time_agg = end_time - start_time\n",
        "    print(f\"Training complete. Time: {train_time_agg:.2f}s\")\n",
        "\n",
        "    # Evaluate on Validation Set\n",
        "    print(\"\\nEvaluating SVM model on validation set (aggregated data)...\")\n",
        "    y_val_pred_svm_agg = svm_model_agg.predict(X_val_scaled_agg)\n",
        "    y_val_pred_proba_agg = svm_model_agg.predict_proba(X_val_scaled_agg)[:, 1]\n",
        "    auc_agg = roc_auc_score(y_val_agg, y_val_pred_proba_agg)\n",
        "    accuracy_agg = accuracy_score(y_val_agg, y_val_pred_svm_agg)\n",
        "    results_agg_only['SVM (Aggregated Only)'] = auc_agg\n",
        "\n",
        "    print(f\"Validation Accuracy (SVM Aggregated): {accuracy_agg:.4f}\")\n",
        "    print(f\"Validation AUC-ROC (SVM Aggregated): {auc_agg:.4f}\")\n",
        "    print(\"\\nValidation Classification Report (SVM Aggregated):\")\n",
        "    try:\n",
        "        print(classification_report(y_val_agg, y_val_pred_svm_agg))\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not generate classification report: {e}\")\n",
        "\n",
        "    # --- Optional: Create Submission File ---\n",
        "    create_submission = True # Set to False if you don't need a submission file now\n",
        "    if create_submission:\n",
        "        print(\"\\nPredicting probabilities on the test set (aggregated data)...\")\n",
        "        y_test_pred_proba_svm_agg = svm_model_agg.predict_proba(X_test_scaled_agg)[:, 1]\n",
        "        print(\"Prediction complete.\")\n",
        "\n",
        "        print(f\"Number of test predictions: {len(y_test_pred_proba_svm_agg)}\")\n",
        "        print(f\"Number of test IDs: {len(test_ids_agg)}\")\n",
        "\n",
        "        if len(test_ids_agg) != len(y_test_pred_proba_svm_agg):\n",
        "             print(f\"Error: Length mismatch! Test IDs ({len(test_ids_agg)}) vs Predictions ({len(y_test_pred_proba_svm_agg)})\")\n",
        "             # Add fallback logic if necessary, similar to previous examples\n",
        "        else:\n",
        "            print(\"Test IDs and predictions match.\")\n",
        "            submission_df_svm_agg = pd.DataFrame({\n",
        "                'id': test_ids_agg,\n",
        "                'readmitted_within_30days': y_test_pred_proba_svm_agg\n",
        "            })\n",
        "            output_path_svm_agg = \"submission_svm_aggregated.csv\"\n",
        "            submission_df_svm_agg.to_csv(output_path_svm_agg, index=False)\n",
        "            print(f\"\\nSubmission file for SVM (Aggregated) created successfully at: {output_path_svm_agg}\")\n",
        "            print(\"Submission file head (SVM Aggregated):\")\n",
        "            print(submission_df_svm_agg.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n!!! An error occurred: {e} !!!\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\n--- Comparison Point ---\")\n",
        "print(f\"Validation AUC for SVM on Aggregated Data (171 features): {results_agg_only.get('SVM (Aggregated Only)', 'N/A'):.4f}\")\n",
        "print(\"Compare this to the AUC score you obtained for SVM after full preprocessing.\")\n",
        "print(\"-\" * 26)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uqEx6r5-nUss",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqEx6r5-nUss",
        "outputId": "3a8a835a-2cd7-49a0-e44c-516385a333ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Comparing SVM (Balanced vs. Unbalanced) on Aggregated Data (No Filtering) ---\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Aggregating features...\n",
            "Aggregating features for train_df using 'mean' method...\n",
            "Aggregation complete for train_df. Shape: (8234, 171)\n",
            "Aggregating features for val_df using 'mean' method...\n",
            "Aggregation complete for val_df. Shape: (2788, 171)\n",
            "Aggregating features for test_df using 'mean' method...\n",
            "Aggregation complete for test_df. Shape: (2741, 171)\n",
            "Aggregated shapes: Train=(8234, 171), Val=(2788, 171), Test=(2741, 171)\n",
            "Scaling aggregated features...\n",
            "Scaling complete.\n",
            "\n",
            "--- Training SVM WITHOUT class_weight='balanced' ---\n",
            "(Note: SVM training with probability=True can be slow)...\n",
            "Training complete. Time: 26.00s\n",
            "\n",
            "Evaluating UNBALANCED SVM model on validation set...\n",
            "Validation Accuracy (SVM Unbalanced): 0.8468\n",
            "Validation AUC-ROC (SVM Unbalanced): 0.7176\n",
            "\n",
            "Validation Classification Report (SVM Unbalanced):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.85      1.00      0.92      2307\n",
            "        True       0.90      0.13      0.22       481\n",
            "\n",
            "    accuracy                           0.85      2788\n",
            "   macro avg       0.87      0.56      0.57      2788\n",
            "weighted avg       0.85      0.85      0.80      2788\n",
            "\n",
            "\n",
            "--- Training SVM WITH class_weight='balanced' ---\n",
            "(Note: SVM training with probability=True can be slow)...\n",
            "Training complete. Time: 40.15s\n",
            "\n",
            "Evaluating BALANCED SVM model on validation set...\n",
            "Validation Accuracy (SVM Balanced): 0.7816\n",
            "Validation AUC-ROC (SVM Balanced): 0.7343\n",
            "\n",
            "Validation Classification Report (SVM Balanced):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.89      0.84      0.86      2307\n",
            "        True       0.39      0.49      0.44       481\n",
            "\n",
            "    accuracy                           0.78      2788\n",
            "   macro avg       0.64      0.67      0.65      2788\n",
            "weighted avg       0.80      0.78      0.79      2788\n",
            "\n",
            "\n",
            "Predicting probabilities on the test set using BALANCED SVM...\n",
            "Prediction complete.\n",
            "Number of test predictions: 2741\n",
            "Number of test IDs: 2741\n",
            "Test IDs and predictions match.\n",
            "\n",
            "Submission file for SVM (Aggregated, Balanced) created successfully at: submission_svm_aggregated_balanced.csv\n",
            "Submission file head (SVM Aggregated, Balanced):\n",
            "                  id  readmitted_within_30days\n",
            "0  16026764_21404901                  0.158666\n",
            "1  18463717_24608289                  0.543315\n",
            "2  16914073_28403863                  0.208099\n",
            "3  17119335_28722930                  0.057497\n",
            "4  10599715_20082646                  0.081485\n",
            "\n",
            "--- SVM AUC Comparison (Aggregated Data Only) ---\n",
            "Aggregation Method: mean\n",
            "--------------------------------------------------\n",
            "SVM (Unbalanced): 0.7176\n",
            "SVM (Balanced):   0.7343\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# 1. Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
        "import warnings\n",
        "\n",
        "# Suppress specific FutureWarning from pandas\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "AGGREGATION_METHOD = 'mean' # Or 'mean', 'max' if you want to test that aggregation\n",
        "# Define file paths globally\n",
        "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
        "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
        "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
        "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
        "\n",
        "# --- Function Definition (Aggregation Step Only) ---\n",
        "def aggregate_admission_data(df, ehr_data_dict, aggregation, df_name=\"DataFrame\", has_labels=True):\n",
        "    \"\"\"Aggregate features per admission with optional labels\"\"\"\n",
        "    print(f\"Aggregating features for {df_name} using '{aggregation}' method...\")\n",
        "    admission_ids = df['id'].unique()\n",
        "    X = []\n",
        "    y = [] if has_labels else None\n",
        "    processed_ids = []\n",
        "    feature_dict = ehr_data_dict.get('feat_dict', {})\n",
        "    feature_cols = ehr_data_dict.get('feature_cols', [])\n",
        "    expected_dim = len(feature_cols)\n",
        "\n",
        "    for adm_id in admission_ids:\n",
        "        if adm_id not in feature_dict: continue\n",
        "        adm_features = feature_dict[adm_id]\n",
        "        if not isinstance(adm_features, np.ndarray) or adm_features.size == 0: continue\n",
        "        try:\n",
        "            if aggregation == 'last': features = adm_features[-1]\n",
        "            elif aggregation == 'mean': features = np.mean(adm_features, axis=0)\n",
        "            elif aggregation == 'max': features = np.max(adm_features, axis=0)\n",
        "            else: raise ValueError(f\"Invalid aggregation method: {aggregation}\")\n",
        "        except Exception: continue\n",
        "        if expected_dim > 0 and features.shape[0] != expected_dim: continue\n",
        "        X.append(features)\n",
        "        processed_ids.append(adm_id)\n",
        "        if has_labels:\n",
        "            adm_rows = df[df['id'] == adm_id]\n",
        "            if not adm_rows.empty: y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
        "            else: X.pop(); processed_ids.pop()\n",
        "    X_np = np.array(X); y_np = np.array(y) if has_labels and y is not None else None\n",
        "    if X_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent samples after aggregation.\")\n",
        "    if has_labels and y_np is not None and y_np.shape[0] != len(processed_ids): raise RuntimeError(\"Inconsistent labels after aggregation.\")\n",
        "    print(f\"Aggregation complete for {df_name}. Shape: {X_np.shape}\")\n",
        "    return X_np, y_np, processed_ids\n",
        "\n",
        "# --- Main Logic ---\n",
        "results_comparison = {} # Store results for comparison\n",
        "\n",
        "try:\n",
        "    # === Run SVM Comparison on Aggregated Data (No Filtering) ===\n",
        "    print(\"--- Comparing SVM (Balanced vs. Unbalanced) on Aggregated Data (No Filtering) ---\")\n",
        "    # 1a. Load Data\n",
        "    print(\"Loading data...\")\n",
        "    train_df = pd.read_csv(train_csv_file)\n",
        "    val_df = pd.read_csv(val_csv_file)\n",
        "    test_df = pd.read_csv(test_csv_file) # Load test_df for potential submission later\n",
        "    with open(ehr_pkl_file, 'rb') as f:\n",
        "        ehr_data = pd.read_pickle(f)\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    # 1b. Aggregate Features\n",
        "    print(\"Aggregating features...\")\n",
        "    X_train_agg, y_train_agg, train_ids_agg = aggregate_admission_data(train_df, ehr_data, AGGREGATION_METHOD, df_name=\"train_df\")\n",
        "    X_val_agg, y_val_agg, val_ids_agg = aggregate_admission_data(val_df, ehr_data, AGGREGATION_METHOD, df_name=\"val_df\")\n",
        "    X_test_agg, _, test_ids_agg = aggregate_admission_data(test_df, ehr_data, AGGREGATION_METHOD, df_name=\"test_df\", has_labels=False)\n",
        "    print(f\"Aggregated shapes: Train={X_train_agg.shape}, Val={X_val_agg.shape}, Test={X_test_agg.shape}\")\n",
        "\n",
        "    # 1c. Scale Aggregated Features\n",
        "    print(\"Scaling aggregated features...\")\n",
        "    scaler_agg = StandardScaler()\n",
        "    X_train_scaled_agg = scaler_agg.fit_transform(X_train_agg)\n",
        "    X_val_scaled_agg = scaler_agg.transform(X_val_agg)\n",
        "    X_test_scaled_agg = scaler_agg.transform(X_test_agg) # Scale test set too\n",
        "    print(\"Scaling complete.\")\n",
        "\n",
        "    # --- Run 1: SVM WITHOUT Balancing ---\n",
        "    print(\"\\n--- Training SVM WITHOUT class_weight='balanced' ---\")\n",
        "    print(\"(Note: SVM training with probability=True can be slow)...\")\n",
        "    svm_model_unbalanced = SVC(random_state=42, C=1.0, kernel='rbf', gamma='scale',\n",
        "                               probability=True, class_weight=None) # Explicitly None\n",
        "    start_time = time.time()\n",
        "    svm_model_unbalanced.fit(X_train_scaled_agg, y_train_agg)\n",
        "    end_time = time.time()\n",
        "    print(f\"Training complete. Time: {end_time - start_time:.2f}s\")\n",
        "\n",
        "    # Evaluate Unbalanced SVM\n",
        "    print(\"\\nEvaluating UNBALANCED SVM model on validation set...\")\n",
        "    y_val_pred_svm_unbalanced = svm_model_unbalanced.predict(X_val_scaled_agg)\n",
        "    y_val_pred_proba_unbalanced = svm_model_unbalanced.predict_proba(X_val_scaled_agg)[:, 1]\n",
        "    auc_unbalanced = roc_auc_score(y_val_agg, y_val_pred_proba_unbalanced)\n",
        "    accuracy_unbalanced = accuracy_score(y_val_agg, y_val_pred_svm_unbalanced)\n",
        "    results_comparison['SVM (Unbalanced)'] = auc_unbalanced\n",
        "\n",
        "    print(f\"Validation Accuracy (SVM Unbalanced): {accuracy_unbalanced:.4f}\")\n",
        "    print(f\"Validation AUC-ROC (SVM Unbalanced): {auc_unbalanced:.4f}\")\n",
        "    print(\"\\nValidation Classification Report (SVM Unbalanced):\")\n",
        "    try:\n",
        "        print(classification_report(y_val_agg, y_val_pred_svm_unbalanced))\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not generate classification report: {e}\")\n",
        "\n",
        "    # --- Run 2: SVM WITH Balancing ---\n",
        "    print(\"\\n--- Training SVM WITH class_weight='balanced' ---\")\n",
        "    print(\"(Note: SVM training with probability=True can be slow)...\")\n",
        "    svm_model_balanced = SVC(random_state=42, C=1.0, kernel='rbf', gamma='scale',\n",
        "                             probability=True, class_weight='balanced') # Use balanced\n",
        "    start_time = time.time()\n",
        "    svm_model_balanced.fit(X_train_scaled_agg, y_train_agg)\n",
        "    end_time = time.time()\n",
        "    train_time_balanced = end_time - start_time # Store time for balanced model\n",
        "    print(f\"Training complete. Time: {train_time_balanced:.2f}s\")\n",
        "\n",
        "    # Evaluate Balanced SVM\n",
        "    print(\"\\nEvaluating BALANCED SVM model on validation set...\")\n",
        "    y_val_pred_svm_balanced = svm_model_balanced.predict(X_val_scaled_agg)\n",
        "    y_val_pred_proba_balanced = svm_model_balanced.predict_proba(X_val_scaled_agg)[:, 1]\n",
        "    auc_balanced = roc_auc_score(y_val_agg, y_val_pred_proba_balanced)\n",
        "    accuracy_balanced = accuracy_score(y_val_agg, y_val_pred_svm_balanced)\n",
        "    results_comparison['SVM (Balanced)'] = auc_balanced\n",
        "\n",
        "    print(f\"Validation Accuracy (SVM Balanced): {accuracy_balanced:.4f}\")\n",
        "    print(f\"Validation AUC-ROC (SVM Balanced): {auc_balanced:.4f}\")\n",
        "    print(\"\\nValidation Classification Report (SVM Balanced):\")\n",
        "    try:\n",
        "        print(classification_report(y_val_agg, y_val_pred_svm_balanced))\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not generate classification report: {e}\")\n",
        "\n",
        "\n",
        "    # --- Optional: Create Submission File (Using the Balanced Model Results) ---\n",
        "    create_submission = True # Set to False if you don't need a submission file now\n",
        "    if create_submission:\n",
        "        print(\"\\nPredicting probabilities on the test set using BALANCED SVM...\")\n",
        "        # Use the balanced model for prediction if creating submission\n",
        "        y_test_pred_proba_svm_balanced = svm_model_balanced.predict_proba(X_test_scaled_agg)[:, 1]\n",
        "        print(\"Prediction complete.\")\n",
        "\n",
        "        print(f\"Number of test predictions: {len(y_test_pred_proba_svm_balanced)}\")\n",
        "        print(f\"Number of test IDs: {len(test_ids_agg)}\")\n",
        "\n",
        "        if len(test_ids_agg) != len(y_test_pred_proba_svm_balanced):\n",
        "             print(f\"Error: Length mismatch! Test IDs ({len(test_ids_agg)}) vs Predictions ({len(y_test_pred_proba_svm_balanced)})\")\n",
        "             # Add fallback logic if necessary\n",
        "        else:\n",
        "            print(\"Test IDs and predictions match.\")\n",
        "            submission_df_svm_balanced = pd.DataFrame({\n",
        "                'id': test_ids_agg,\n",
        "                'readmitted_within_30days': y_test_pred_proba_svm_balanced\n",
        "            })\n",
        "            output_path_svm_balanced = \"submission_svm_aggregated_balanced.csv\" # Specific name\n",
        "            submission_df_svm_balanced.to_csv(output_path_svm_balanced, index=False)\n",
        "            print(f\"\\nSubmission file for SVM (Aggregated, Balanced) created successfully at: {output_path_svm_balanced}\")\n",
        "            print(\"Submission file head (SVM Aggregated, Balanced):\")\n",
        "            print(submission_df_svm_balanced.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n!!! An error occurred: {e} !!!\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# --- Final Comparison Output ---\n",
        "print(\"\\n--- SVM AUC Comparison (Aggregated Data Only) ---\")\n",
        "print(f\"Aggregation Method: {AGGREGATION_METHOD}\")\n",
        "print(\"-\" * 50)\n",
        "auc_unbal = results_comparison.get('SVM (Unbalanced)', 'N/A')\n",
        "auc_bal = results_comparison.get('SVM (Balanced)', 'N/A')\n",
        "\n",
        "print(f\"SVM (Unbalanced): {auc_unbal:.4f}\" if isinstance(auc_unbal, float) else f\"SVM (Unbalanced): {auc_unbal}\")\n",
        "print(f\"SVM (Balanced):   {auc_bal:.4f}\" if isinstance(auc_bal, float) else f\"SVM (Balanced):   {auc_bal}\")\n",
        "print(\"-\" * 50)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 7241590,
          "sourceId": 11547483,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31011,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
