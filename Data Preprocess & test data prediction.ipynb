{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57700c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns                       #visualisation\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change this to yours ...\n",
    "DATA_DIR = \"/\"\n",
    "\n",
    "# read 3 csv files and 1 pkl file\n",
    "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
    "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
    "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "val_df = pd.read_csv(val_csv_file)\n",
    "test_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "test_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "\n",
    "\n",
    "with open(ehr_pkl_file, 'rb') as f:\n",
    "    ehr_data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e09a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_admission_data(df, ehr_dict, aggregation, has_labels=True):\n",
    "    \"\"\"Aggregate features per admission with optional labels\"\"\"\n",
    "    admission_ids = df['id'].unique()\n",
    "    X = []\n",
    "    y = [] if has_labels else None\n",
    "    \n",
    "    for adm_id in admission_ids:\n",
    "        adm_rows = df[df['id'] == adm_id]\n",
    "        adm_features = ehr_dict[adm_id]\n",
    "        \n",
    "        if aggregation == 'last':\n",
    "            features = adm_features[-1]\n",
    "        elif aggregation == 'mean':\n",
    "            features = np.mean(adm_features, axis=0)\n",
    "        elif aggregation == 'max':\n",
    "            features = np.max(adm_features, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation method\")\n",
    "        \n",
    "        X.append(features)\n",
    "        \n",
    "        if has_labels:\n",
    "            y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
    "    \n",
    "    return np.array(X), (np.array(y) if has_labels else None)\n",
    "\n",
    "# Process datasets\n",
    "X_train, y_train = aggregate_admission_data(train_df, ehr_data[\"feat_dict\"], 'last') # change last/max/mean\n",
    "X_val, y_val = aggregate_admission_data(val_df, ehr_data[\"feat_dict\"], 'last') \n",
    "X_test = aggregate_admission_data(test_df, ehr_data[\"feat_dict\"], 'last', has_labels=False)[0]\n",
    "\n",
    "print(f\"Shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_feature_associations(X, y, feature_names, ehr_data):\n",
    "    correlations = []\n",
    "    p_values = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        if np.std(X[:, i]) == 0:  \n",
    "            correlations.append(0)\n",
    "            p_values.append(1)\n",
    "            continue\n",
    "            \n",
    "        if feature_names[i] in ehr_data[\"icd_cols\"]:\n",
    "            # Chi-squared for ICD features\n",
    "            contingency = pd.crosstab(X[:, i], y)\n",
    "            try:\n",
    "                chi2, pval, _, _ = chi2_contingency(contingency)\n",
    "                correlations.append(chi2)\n",
    "                p_values.append(pval)\n",
    "            except:\n",
    "                correlations.append(0)\n",
    "                p_values.append(1)\n",
    "        else:\n",
    "            # Point-biserial for continuous features\n",
    "            corr, pval = pointbiserialr(X[:, i], y)\n",
    "            correlations.append(corr)\n",
    "            p_values.append(pval)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Association': correlations,\n",
    "        'P-value': p_values\n",
    "    })\n",
    "\n",
    "# 2. Function to identify highly correlated features\n",
    "def find_collinear_features(X, feature_names, threshold=0.8):\n",
    "    \"\"\"Identify feature pairs with correlation > threshold\"\"\"\n",
    "    corr_matrix = pd.DataFrame(X).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    collinear_pairs = [(feature_names[i], feature_names[j], corr_matrix.iloc[i,j]) \n",
    "                      for i,j in zip(*np.where(upper > threshold))]\n",
    "    return pd.DataFrame(collinear_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
    "\n",
    "# 3. Feature selection based on clinical relevance and collinearity\n",
    "def select_non_collinear_features(X, feature_names, ehr_data, \n",
    "                                clinically_relevant, corr_threshold=0.8):\n",
    "    # Standardize data for proper correlation calculation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Find all collinear pairs\n",
    "    collinear_df = find_collinear_features(X_scaled, feature_names, corr_threshold)\n",
    "    \n",
    "    # Get feature importance scores\n",
    "    importance = mutual_info_classif(X, y_train, random_state=42)\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "    \n",
    "    features_to_keep = set(clinically_relevant)  # Always keep these\n",
    "    \n",
    "    # For non-clinical features, keep the more important one from collinear pairs\n",
    "    for _, row in collinear_df.iterrows():\n",
    "        f1, f2 = row['Feature1'], row['Feature2']\n",
    "        \n",
    "        # Skip if either is clinically relevant\n",
    "        if f1 in clinically_relevant or f2 in clinically_relevant:\n",
    "            continue\n",
    "            \n",
    "        # Keep feature with higher importance\n",
    "        imp1 = importance_df.loc[importance_df['Feature'] == f1, 'Importance'].values[0]\n",
    "        imp2 = importance_df.loc[importance_df['Feature'] == f2, 'Importance'].values[0]\n",
    "        \n",
    "        if imp1 > imp2:\n",
    "            features_to_keep.add(f1)\n",
    "        else:\n",
    "            features_to_keep.add(f2)\n",
    "    \n",
    "    # Add any features not involved in collinearity\n",
    "    all_features = set(feature_names)\n",
    "    features_to_keep.update(all_features - set(collinear_df[['Feature1','Feature2']].values.flatten()))\n",
    "    \n",
    "    # Get indices of features to keep\n",
    "    keep_indices = [i for i, f in enumerate(feature_names) if f in features_to_keep]\n",
    "    remaining_features = [f for f in feature_names if f in features_to_keep]\n",
    "    \n",
    "    return keep_indices, remaining_features\n",
    "\n",
    "\n",
    "\n",
    "# 1. Compute associations with target\n",
    "assoc_df = compute_feature_associations(X_train_filtered, y_train, remaining_features, ehr_data)\n",
    "\n",
    "# 2. Find and handle collinearity\n",
    "keep_indices, non_collinear_features = select_non_collinear_features(\n",
    "    X_train_filtered,\n",
    "    remaining_features,\n",
    "    ehr_data,\n",
    "    clinically_relevant,\n",
    "    corr_threshold=0.7  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# 3. Apply final filtering\n",
    "X_train_final = X_train_filtered[:, keep_indices]\n",
    "X_val_final = X_val_filtered[:, keep_indices]\n",
    "X_test_final = X_test_filtered[:, keep_indices]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your training process here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9df95f",
   "metadata": {},
   "source": [
    "Predict test data by fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2621f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change model name to the fitted model name you initiated i.e. rf_model.predict(X_test)\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(y_test_pred)\n",
    "len(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960f77a",
   "metadata": {},
   "source": [
    "generate a test result to your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f34251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = y_test_pred.astype(int)\n",
    "\n",
    "\n",
    "print(y_test_pred)\n",
    "\n",
    "\n",
    "test_df = test_df.drop_duplicates(subset='id', keep='first')\n",
    "class_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'readmitted_within_30days': y_test_pred\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dir = '/enter your output path'\n",
    "\n",
    "class_df.to_csv(f'{output_dir}class_predictions.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Probability predictions saved with duplicates handled:\")\n",
    "\n",
    "print(\"\\nClass predictions saved with duplicates handled:\")\n",
    "print(class_df.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
