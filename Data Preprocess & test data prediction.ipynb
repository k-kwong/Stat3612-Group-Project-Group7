{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f57700c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns                       #visualisation\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e94906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change this to yours ...\n",
    "DATA_DIR = \"/Users/kwongtszkong/Desktop/STAT3612 Statistical ML/Group Project/Stat3612_Project_datasource\"\n",
    "\n",
    "# read 3 csv files and 1 pkl file\n",
    "train_csv_file = os.path.join(DATA_DIR, \"train.csv\")\n",
    "val_csv_file = os.path.join(DATA_DIR, \"valid.csv\")\n",
    "test_csv_file = os.path.join(DATA_DIR, \"test.csv\")\n",
    "ehr_pkl_file = os.path.join(DATA_DIR, \"ehr_preprocessed_seq_by_day_cat_embedding.pkl\")\n",
    "\n",
    "train_df = pd.read_csv(train_csv_file)\n",
    "val_df = pd.read_csv(val_csv_file)\n",
    "test_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "test_df = pd.read_csv(test_csv_file)\n",
    "\n",
    "\n",
    "\n",
    "with open(ehr_pkl_file, 'rb') as f:\n",
    "    ehr_data = pd.read_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "738e09a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - Train: (8234, 171), Val: (2788, 171), Test: (2741, 171)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def aggregate_admission_data(df, ehr_dict, aggregation, has_labels=True):\n",
    "    \"\"\"Aggregate features per admission with optional labels\"\"\"\n",
    "    admission_ids = df['id'].unique()\n",
    "    X = []\n",
    "    y = [] if has_labels else None\n",
    "    \n",
    "    for adm_id in admission_ids:\n",
    "        adm_rows = df[df['id'] == adm_id]\n",
    "        adm_features = ehr_dict[adm_id]\n",
    "        \n",
    "        if aggregation == 'last':\n",
    "            features = adm_features[-1]\n",
    "        elif aggregation == 'mean':\n",
    "            features = np.mean(adm_features, axis=0)\n",
    "        elif aggregation == 'max':\n",
    "            features = np.max(adm_features, axis=0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid aggregation method\")\n",
    "        \n",
    "        X.append(features)\n",
    "        \n",
    "        if has_labels:\n",
    "            y.append(adm_rows['readmitted_within_30days'].iloc[0])\n",
    "    \n",
    "    return np.array(X), (np.array(y) if has_labels else None)\n",
    "\n",
    "# Process datasets\n",
    "X_train, y_train = aggregate_admission_data(train_df, ehr_data[\"feat_dict\"], 'last') # change last/max/mean\n",
    "X_val, y_val = aggregate_admission_data(val_df, ehr_data[\"feat_dict\"], 'last') \n",
    "X_test = aggregate_admission_data(test_df, ehr_data[\"feat_dict\"], 'last', has_labels=False)[0]\n",
    "\n",
    "print(f\"Shapes - Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c13ddef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8234, 127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/xhsm8rlj5fz4b46rjy_ls9t40000gn/T/ipykernel_15246/26910589.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  zero_df = pd.concat([demo_zero_df, icd_zero_df, lab_zero_df, med_zero_df])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "# initialize storage for std_devs\n",
    "std_devs = np.std(X_train, axis=0)\n",
    "\n",
    "\n",
    "feature_names = ehr_data[\"feature_cols\"]  \n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "std_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Std_Dev\": std_devs\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Initialize storage\n",
    "correlations = []\n",
    "p_values = []\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    feature_name = feature_names[i]\n",
    "    \n",
    "    # Skip constant features\n",
    "    if np.std(X_train[:, i]) == 0:\n",
    "        correlations.append(0)\n",
    "        p_values.append(1)\n",
    "        continue\n",
    "        \n",
    "    # Use chi-squared for ICD features, point-biserial for others\n",
    "    if feature_name in ehr_data[\"icd_cols\"]:\n",
    "        # Chi-squared test for binary ICD features\n",
    "        contingency_table = pd.crosstab(X_train[:, i], y_train)\n",
    "        try:\n",
    "            chi2, pval, _, _ = chi2_contingency(contingency_table)\n",
    "            correlations.append(chi2)  # Using chi2 statistic as effect size\n",
    "            p_values.append(pval)\n",
    "        except:\n",
    "            correlations.append(0)\n",
    "            p_values.append(1)\n",
    "    else:\n",
    "        # Point-biserial for continuous features\n",
    "        corr, pval = pointbiserialr(X_train[:, i], y_train)\n",
    "        correlations.append(corr)\n",
    "        p_values.append(pval)\n",
    "        \n",
    "\n",
    "\n",
    "# Create results DataFrame\n",
    "corr_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Correlation\": correlations,\n",
    "    \"P-value\": p_values\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Sort by absolute correlation strength\n",
    "corr_df[\"Abs_Correlation\"] = np.abs(corr_df[\"Correlation\"])\n",
    "corr_df = corr_df.sort_values(\"Abs_Correlation\", ascending=True)\n",
    "\n",
    "\n",
    "\n",
    "# Function to analyze zero-value ratio in features\n",
    "def analyze_zeros(ehr_data, feature_cols, threshold=0.5):\n",
    "\n",
    "    all_zeros = []\n",
    "    for feature_name, feature_idx in zip(feature_cols, range(len(feature_cols))):\n",
    "        zeros_count = np.sum([np.sum(matrix[:, feature_idx] == 0) for matrix in ehr_data[\"feat_dict\"].values()])\n",
    "        total_values = sum([matrix.shape[0] for matrix in ehr_data[\"feat_dict\"].values()])\n",
    "        zero_ratio = zeros_count / total_values\n",
    "        if zero_ratio > threshold:\n",
    "            all_zeros.append((feature_name, zero_ratio))\n",
    "    return pd.DataFrame(all_zeros, columns=[\"Feature\", \"Zero_Ratio\"])\n",
    "\n",
    "# Analyze zero-value ratio for each feature category\n",
    "demo_zero_df = analyze_zeros(ehr_data, ehr_data[\"demo_cols\"])\n",
    "icd_zero_df = analyze_zeros(ehr_data, ehr_data[\"icd_cols\"])\n",
    "lab_zero_df = analyze_zeros(ehr_data, ehr_data[\"lab_cols\"])\n",
    "med_zero_df = analyze_zeros(ehr_data, ehr_data[\"med_cols\"])\n",
    "\n",
    "# Combine all zero-value data\n",
    "zero_df = pd.concat([demo_zero_df, icd_zero_df, lab_zero_df, med_zero_df])\n",
    "\n",
    "# Merge zero-value data with std and correlation data\n",
    "combined_df = pd.merge(pd.merge(std_df, corr_df, on='Feature'), zero_df, on='Feature', how='left')\n",
    "\n",
    "\n",
    "\n",
    "clinically_relevant = [\n",
    "    # Labs\n",
    "    'Creatinine Blood', 'Hemoglobin Blood', 'Hematocrit Blood',\n",
    "    'Potassium Blood', 'Sodium Blood', 'Glucose Blood',\n",
    "    'Troponin T Blood', 'Platelet Count Blood', 'Eosinophils Blood',\n",
    "    'pH Urine', 'pO2 Blood', 'pCO2 Blood', 'Anion Gap Blood',\n",
    "    \n",
    "    # ICD-10\n",
    "    'I10-I16', 'N17-N19', 'J09-J18', 'E70-E88', 'I30-I52',\n",
    "    'J40-J47', 'B20-B20',\n",
    "    \n",
    "    # Drugs\n",
    "    'ANTICOAGULANTS', 'ANTIBIOTICS', 'IMMUNOSUPPRESSANTS',\n",
    "    'ANTIINFLAM.TUMOR NECROSIS FACTOR INHIBITING AGENTS'\n",
    "]\n",
    "\n",
    "\n",
    "thresholds = {\n",
    "    'zero_ratio': {\n",
    "        'icd': 0.95,    # ICD codes often sparse\n",
    "        'med': 0.95,    # Medications often sparse\n",
    "        'lab': 0.95,    # Labs should rarely be zero\n",
    "        'demo': 0.9     # Demographics rarely zero\n",
    "    },\n",
    "    'std_dev': 0.001,    # Only for continuous features\n",
    "    'correlation': {\n",
    "        'min_abs_corr': 0.001,\n",
    "        'max_pvalue': 0.05\n",
    "    }\n",
    "}\n",
    "\n",
    "# Categorize features\n",
    "feature_types = {\n",
    "    'icd': ehr_data[\"icd_cols\"],\n",
    "    'med': ehr_data[\"med_cols\"],\n",
    "    'lab': ehr_data[\"lab_cols\"],\n",
    "    'demo': ehr_data[\"demo_cols\"]\n",
    "}\n",
    "\n",
    "# Initialize storage\n",
    "features_to_remove = []\n",
    "\n",
    "\n",
    "# Check each feature type separately\n",
    "for ftype, cols in feature_types.items():\n",
    "    for feature in cols:\n",
    "        row = combined_df[combined_df['Feature'] == feature].iloc[0]\n",
    "        \n",
    "        # Skip binary features for variance check\n",
    "        check_variance = ftype not in ['icd']\n",
    "        \n",
    "        if ftype == 'icd':\n",
    "            sig_threshold = 0.05  # Could use different threshold for chi-square\n",
    "            effect_size = row['Correlation']  # Actually contains chi2 statistic\n",
    "        else:\n",
    "            sig_threshold = thresholds['correlation']['max_pvalue']\n",
    "            effect_size = np.abs(row['Correlation'])\n",
    "        \n",
    "        if (row['Zero_Ratio'] > thresholds['zero_ratio'][ftype]) and \\\n",
    "           (not check_variance or row['Std_Dev'] < thresholds['std_dev']) and \\\n",
    "           (effect_size < thresholds['correlation']['min_abs_corr']) and \\\n",
    "           (row['P-value'] > sig_threshold):\n",
    "            if feature not in clinically_relevant:\n",
    "                features_to_remove.append(feature)\n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "filtered_df = combined_df[~combined_df['Feature'].isin(clinically_relevant)]\n",
    "# Generate report\n",
    "removal_df = filtered_df[filtered_df['Feature'].isin(features_to_remove)].sort_values(\n",
    "    by=['Zero_Ratio', 'Std_Dev', 'Abs_Correlation'],\n",
    "    ascending=[False, True, True]\n",
    ")\n",
    "\n",
    "# Get indices of features to remove\n",
    "features_to_remove = removal_df['Feature'].tolist()\n",
    "all_features = ehr_data[\"feature_cols\"]\n",
    "remove_indices = [i for i, feature in enumerate(all_features) \n",
    "                 if feature in features_to_remove]\n",
    "\n",
    "#  Function to remove features\n",
    "def remove_features(X, remove_indices):\n",
    "    return np.delete(X, remove_indices, axis=1)\n",
    "\n",
    "#  Apply to both training and validation sets\n",
    "X_train_filtered = remove_features(X_train, remove_indices)\n",
    "X_val_filtered = remove_features(X_val, remove_indices)\n",
    "X_test_filtered =remove_features(X_test, remove_indices)\n",
    "\n",
    "# 4. Get remaining feature names\n",
    "remaining_features = [f for i, f in enumerate(all_features) \n",
    "                     if i not in remove_indices]\n",
    "\n",
    "\n",
    "\n",
    "print(X_train_filtered.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c343aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pointbiserialr, chi2_contingency\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_feature_associations(X, y, feature_names, ehr_data):\n",
    "    correlations = []\n",
    "    p_values = []\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        if np.std(X[:, i]) == 0:  \n",
    "            correlations.append(0)\n",
    "            p_values.append(1)\n",
    "            continue\n",
    "            \n",
    "        if feature_names[i] in ehr_data[\"icd_cols\"]:\n",
    "            # Chi-squared for ICD features\n",
    "            contingency = pd.crosstab(X[:, i], y)\n",
    "            try:\n",
    "                chi2, pval, _, _ = chi2_contingency(contingency)\n",
    "                correlations.append(chi2)\n",
    "                p_values.append(pval)\n",
    "            except:\n",
    "                correlations.append(0)\n",
    "                p_values.append(1)\n",
    "        else:\n",
    "            # Point-biserial for continuous features\n",
    "            corr, pval = pointbiserialr(X[:, i], y)\n",
    "            correlations.append(corr)\n",
    "            p_values.append(pval)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Association': correlations,\n",
    "        'P-value': p_values\n",
    "    })\n",
    "\n",
    "# 2. Function to identify highly correlated features\n",
    "def find_collinear_features(X, feature_names, threshold=0.8):\n",
    "    \"\"\"Identify feature pairs with correlation > threshold\"\"\"\n",
    "    corr_matrix = pd.DataFrame(X).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    collinear_pairs = [(feature_names[i], feature_names[j], corr_matrix.iloc[i,j]) \n",
    "                      for i,j in zip(*np.where(upper > threshold))]\n",
    "    return pd.DataFrame(collinear_pairs, columns=['Feature1', 'Feature2', 'Correlation'])\n",
    "\n",
    "# 3. Feature selection based on clinical relevance and collinearity\n",
    "def select_non_collinear_features(X, feature_names, ehr_data, \n",
    "                                clinically_relevant, corr_threshold=0.8):\n",
    "    # Standardize data for proper correlation calculation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Find all collinear pairs\n",
    "    collinear_df = find_collinear_features(X_scaled, feature_names, corr_threshold)\n",
    "    \n",
    "    # Get feature importance scores\n",
    "    importance = mutual_info_classif(X, y_train, random_state=42)\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "    \n",
    "    features_to_keep = set(clinically_relevant)  # Always keep these\n",
    "    \n",
    "    # For non-clinical features, keep the more important one from collinear pairs\n",
    "    for _, row in collinear_df.iterrows():\n",
    "        f1, f2 = row['Feature1'], row['Feature2']\n",
    "        \n",
    "        # Skip if either is clinically relevant\n",
    "        if f1 in clinically_relevant or f2 in clinically_relevant:\n",
    "            continue\n",
    "            \n",
    "        # Keep feature with higher importance\n",
    "        imp1 = importance_df.loc[importance_df['Feature'] == f1, 'Importance'].values[0]\n",
    "        imp2 = importance_df.loc[importance_df['Feature'] == f2, 'Importance'].values[0]\n",
    "        \n",
    "        if imp1 > imp2:\n",
    "            features_to_keep.add(f1)\n",
    "        else:\n",
    "            features_to_keep.add(f2)\n",
    "    \n",
    "    # Add any features not involved in collinearity\n",
    "    all_features = set(feature_names)\n",
    "    features_to_keep.update(all_features - set(collinear_df[['Feature1','Feature2']].values.flatten()))\n",
    "    \n",
    "    # Get indices of features to keep\n",
    "    keep_indices = [i for i, f in enumerate(feature_names) if f in features_to_keep]\n",
    "    remaining_features = [f for f in feature_names if f in features_to_keep]\n",
    "    \n",
    "    return keep_indices, remaining_features\n",
    "\n",
    "\n",
    "\n",
    "# 1. Compute associations with target\n",
    "assoc_df = compute_feature_associations(X_train_filtered, y_train, remaining_features, ehr_data)\n",
    "\n",
    "# 2. Find and handle collinearity\n",
    "keep_indices, non_collinear_features = select_non_collinear_features(\n",
    "    X_train_filtered,\n",
    "    remaining_features,\n",
    "    ehr_data,\n",
    "    clinically_relevant,\n",
    "    corr_threshold=0.7  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# 3. Apply final filtering\n",
    "X_train_final = X_train_filtered[:, keep_indices]\n",
    "X_val_final = X_val_filtered[:, keep_indices]\n",
    "X_test_final = X_test_filtered[:, keep_indices]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dce974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add your training process here\n",
    "#you can directly use (X_train,X_val) or (X_train_final, X_train_val) to do training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9df95f",
   "metadata": {},
   "source": [
    "Predict test data by fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2621f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2741"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please change model name to the fitted model name you initiated i.e. rf_model.predict(X_test/X_final)\n",
    "y_test_pred = elastic_net.predict(X_test_final)\n",
    "print(y_test_pred)\n",
    "len(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6960f77a",
   "metadata": {},
   "source": [
    "generate a test result to your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed5733",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_pred = y_test_pred.astype(int)\n",
    "\n",
    "\n",
    "print(y_test_pred)\n",
    "\n",
    "\n",
    "test_df = test_df.drop_duplicates(subset='id', keep='first')\n",
    "class_df = pd.DataFrame({\n",
    "    'id': test_df['id'],\n",
    "    'readmitted_within_30days': y_test_pred\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "output_dir = '/enter your output path'\n",
    "\n",
    "class_df.to_csv(f'{output_dir}class_predictions.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Probability predictions saved with duplicates handled:\")\n",
    "\n",
    "print(\"\\nClass predictions saved with duplicates handled:\")\n",
    "print(class_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
